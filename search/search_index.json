{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;","pipeline":["stopWordFilter"]},"docs":[{"location":"contributing/","title":"Contributing","text":"<p>For development use the fork the repo on github, then run <pre><code>git clone https://github.com/&lt;your-username&gt;/.git\ncd pymgcv\npixi shell -e dev  # activates the devlopment env\n</code></pre></p>"},{"location":"contributing/#testing","title":"Testing","text":"<p>Run tests with: <pre><code>pytest\n</code></pre></p>"},{"location":"contributing/#documentation","title":"Documentation","text":"<p>The documentation includes notebook examples in docs/examples. To rerun all these prior to building the docs, run <pre><code>pixi run notebooks\n</code></pre></p> <p>We can locally serve the docs using <pre><code>mkdocs serve\n</code></pre></p>"},{"location":"contributing/#adding-code-examples-in-docstrings","title":"Adding code examples in docstrings","text":"<p>In order for the doctest to run properly, and documentation to be generated properly:</p> <ul> <li>Use the admonition <code>!!! example</code>, containing a python fenced code block, i.e.: <pre><code>!!! example\n\n    ```python\n    # Insert runnable example here\n    ```\n</code></pre></li> <li>Do not use a prompt <code>&gt;&gt;&gt;</code> as this interferes with copying code blocks and looks cluttered!</li> <li>We generally do not test the output, just that each example runs (although you could use an assert if desired)</li> <li>Use a comment at the bottom if you want to show an output example</li> </ul>"},{"location":"api/basis_functions/","title":"Basis Functions","text":"<p>Basis functions control how smooth terms behave in your GAM models -- different basis types are suitable for different kinds of data and modeling requirements:</p>"},{"location":"api/basis_functions/#pymgcv.basis_functions.ThinPlateSpline","title":"ThinPlateSpline","text":"<pre><code>ThinPlateSpline(\n    *,\n    shrinkage: bool | None = False,\n    m: int | None = None,\n    max_knots: int | None = None,\n)\n</code></pre> <p>Thin plate regression spline basis.</p> <p>Parameters:</p> <ul> <li> <code>shrinkage</code>               (<code>bool | None</code>, default:                   <code>False</code> )           \u2013            <p>If True, the penalty is modified so that the term is shrunk to zero for a high enough smoothing parameter.</p> </li> <li> <code>m</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The order of the derivative in the thin plate spline penalty. If \\(d\\) is the number of covariates for the smooth term, this must satisfy \\(m&gt;(d+1)/2\\). If left to None, the smallest value satisfying \\(m&gt;(d+1)/2\\) will be used, which creates \"visually smooth\" functions.</p> </li> <li> <code>max_knots</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The maximum number of knots to use. Defaults to 2000.</p> </li> </ul>"},{"location":"api/basis_functions/#pymgcv.basis_functions.CubicSpline","title":"CubicSpline","text":"<pre><code>CubicSpline(*, shrinkage: bool = False, cyclic: bool = False)\n</code></pre> <p>Cubic regression spline basis.</p> <p>Cubic splines use piecewise cubic polynomials with knots placed throughout the data range. They tend to be computationally efficient, but often performs slightly worse than thin plate splines and are limited to univariate smooths. Note the limitation of being restricted to one-dimensional smooths does not imply they cannot be used for multivariate <code>T</code> smooths, which are constructed from marginal bases.</p> <p>Parameters:</p> <ul> <li> <code>cyclic</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, creates a cyclic spline where the function values and derivatives match at the boundaries. Use for periodic data like time of day, angles, or seasonal patterns. Default is False.</p> </li> <li> <code>shrinkage</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, adds penalty to the null space (linear component). Helps with model selection and identifiability. Default is False. Cannot be used with cyclic=True.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If both cyclic and shrinkage are True (incompatible options)</p> </li> </ul>"},{"location":"api/basis_functions/#pymgcv.basis_functions.BSpline","title":"BSpline","text":"<pre><code>BSpline(*, degree: int = 3, penalty_orders: Iterable[int] | None = None)\n</code></pre> <p>B-spline basis with derivative-based penalties.</p> <p>These are univariate (but note univariate smooths can be used for multivariate smooths constructed with <code>T</code>). <code>BSpline(degree=3, penalty_orders=[2])</code> constructs a conventional cubic spline.</p> <p>Parameters:</p> <ul> <li> <code>degree</code>               (<code>int</code>, default:                   <code>3</code> )           \u2013            <p>The degree of the B-spline basis (e.g. 3 for a cubic spline).</p> </li> <li> <code>penalty_orders</code>               (<code>Iterable[int] | None</code>, default:                   <code>None</code> )           \u2013            <p>The derivative orders to penalize. Default to [degree - 1].</p> </li> </ul>"},{"location":"api/basis_functions/#pymgcv.basis_functions.PSpline","title":"PSpline","text":"<pre><code>PSpline(*, degree: int = 3, penalty_order: int | None = None)\n</code></pre> <p>P-spline (penalized spline) basis as proposed by Eilers and Marx (1996).</p> <p>Uses B-spline bases penalized by discrete penalties applied directly to the basis coefficients. Note for most use cases splines with derivative-based penalties (e.g. <code>ThinPlateSpline</code> or <code>CubicSpline</code>) tend to yield better MSE performance. <code>BSpline(degree=3, penalty_order=2)</code> is cubic-spline-like.</p> <p>Parameters:</p> <ul> <li> <code>degree</code>               (<code>int</code>, default:                   <code>3</code> )           \u2013            <p>Degree of the B-spline basis (e.g. 3 for cubic).</p> </li> <li> <code>penalty_order</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The difference order to penalize. 0-th order is ridge penalty. Default to <code>degree-1</code>.</p> </li> </ul>"},{"location":"api/basis_functions/#pymgcv.basis_functions.DuchonSpline","title":"DuchonSpline","text":"<pre><code>DuchonSpline(*, m: int = 2, s: float | int = 0)\n</code></pre> <p>Duchon spline basis - a generalization of thin plate splines.</p> <p>These smoothers allow the use of lower orders of derivative in the penalty than conventional thin plate splines, while still yielding continuous functions.</p> <p>The description, adapted from mgcv is as follows: Duchon\u2019s (1977) construction generalizes the usual thin plate spline penalty as follows. The usual thin plate spline penalty is given by the integral of the squared Euclidian norm of a vector of mixed partial \\(m\\)-th order derivatives of the function w.r.t. its arguments. Duchon re-expresses this penalty in the Fourier domain, and then weights the squared norm in the integral by the Euclidean norm of the fourier frequencies, raised to the power \\(2s\\), where \\(s\\) is a user selected constant.</p> <p>If \\(d\\) is the number of arguments of the smooth:</p> <ul> <li>It is required that \\(-d/2 &lt; s &lt; d/2\\).</li> <li>If \\(s=0\\) then the usual thin plate spline is recovered.</li> <li>To obtain continuous functions we further require that \\(m + s &gt; d/2\\).</li> </ul> <p>For example, <code>DuchonSpline(m=1, s=d/2)</code> can be used in order to use first derivative penalization for any \\(d\\), and still yield continuous functions.</p> <p>Parameters:</p> <ul> <li> <code>m </code>           \u2013            <p>Order of derivative to penalize.</p> </li> <li> <code>s </code>           \u2013            <p>\\(s\\) as described above, should be an integer divided by 2.</p> </li> </ul>"},{"location":"api/basis_functions/#pymgcv.basis_functions.SplineOnSphere","title":"SplineOnSphere","text":"<pre><code>SplineOnSphere(*, m: int = 0)\n</code></pre> <p>Isotropic smooth for data on a sphere (latitude/longitude coordinates).</p> <p>This should be used with exactly two variables, where the first represents latitude on the interval [-90, 90] and the second represents longitude on the interval [-180, 180].</p> <p>Parameters:</p> <ul> <li> <code>m </code>           \u2013            <p>An integer in [-1, 4]. Setting <code>m=-1</code> uses <code>DuchonSpline(m=2,s=1/2)</code>. Setting <code>m=0</code> signals to use the 2nd order spline on the sphere, computed by Wendelberger\u2019s (1981) method. For m&gt;0, (m+2)/2 is the penalty order, with m=2 equivalent to the usual second derivative penalty.</p> </li> </ul>"},{"location":"api/basis_functions/#pymgcv.basis_functions.RandomEffect","title":"RandomEffect","text":"<pre><code>RandomEffect()\n</code></pre> <p>Random effect basis for correlated grouped data.</p> <p>This can be used with any mixture of numeric or categorical variables. Acts similarly to an <code>Interaction</code> but penalizes the corresponding coefficients with a multiple of the identity matrix (i.e. a ridge penalty), corresponding to an assumption of i.i.d. normality of the parameters.</p> <p>Warning</p> <p>Numeric variables (int/float), will be treated as a linear term with a single penalized slope parameter. Do not use an integer variable to encode categorical groups!</p> <p>Example</p> <p>For an example, see the supplement vs placebo example.</p>"},{"location":"api/basis_functions/#pymgcv.basis_functions.MarkovRandomField","title":"MarkovRandomField","text":"<pre><code>MarkovRandomField(*, polys: list[ndarray])\n</code></pre> <p>Markov Random Field basis for discrete spatial data with neighborhood structure.</p> <p>The smoothing penalty encourages similar value in neighboring locations. When using this basis, the variable passed to <code>S</code> should be a categorical variable representing the area labels.</p> <p>Parameters:</p> <ul> <li> <code>polys</code>               (<code>list[ndarray]</code>)           \u2013            <p>List of numpy arrays defining the spatial polygons or neighborhood structure. Each array represents the boundary or connectivity information for a spatial unit.</p> </li> </ul>"},{"location":"api/basis_functions/#pymgcv.basis_functions.FactorSmooth","title":"FactorSmooth","text":"<pre><code>FactorSmooth(bs: AbstractBasis = &lt;factory&gt;)\n</code></pre> <p>S for each level of a categorical variable.</p> <p>When using this basis, the first variable of the smooth should be a numeric variable, and the second should be a categorical variable.</p> <p>Unlike using a categorical by variable e.g. <code>S(x, by=\"group\")</code>:</p> <ul> <li>The terms share a smoothing parameter.</li> <li>The terms are fully penalized, with seperate penalties on each null space     component (e.g. intercepts). The terms are non-centered, and can     be used with an intercept without introducing indeterminacy, due to the     penalization.</li> </ul> <p>Parameters:</p> <ul> <li> <code>bs</code>               (<code>AbstractBasis</code>, default:                   <code>&lt;factory&gt;</code> )           \u2013            <p>Any singly penalized basis function. Defaults to <code>ThinPlateSpline</code>. Only the type of the basis is passed to mgcv (i.e. what is returned by <code>str(bs)</code>). This is a limitation of mgcv (e.g. you cannot do ) mgcv provides no way to pass more details for setting up the basis function.</p> </li> </ul>"},{"location":"api/basis_functions/#pymgcv.basis_functions.AbstractBasis","title":"AbstractBasis","text":"<p>Abstract class defining the interface for GAM basis functions.</p> <p>All basis function classes must implement this protocol to be usable with smooth terms. The protocol ensures basis functions can be converted to appropriate mgcv R syntax and provide any additional parameters needed.</p>"},{"location":"api/families/","title":"Families","text":"<p>Families define the distribution and link functions for GAM models. Each family corresponds to a statistical distribution and supports specific link functions.</p>"},{"location":"api/families/#continuous-response","title":"Continuous response","text":""},{"location":"api/families/#pymgcv.families.Gaussian","title":"Gaussian","text":"<pre><code>Gaussian(link: Literal['identity', 'log', 'inverse'] = 'identity')\n</code></pre> <p>Gaussian family with specified link function.</p> <p>Parameters:</p> <ul> <li> <code>link</code>               (<code>Literal['identity', 'log', 'inverse']</code>, default:                   <code>'identity'</code> )           \u2013            <p>The link function.</p> </li> </ul>"},{"location":"api/families/#pymgcv.families.Gaussian.cdf","title":"cdf","text":"<pre><code>cdf(x: ndarray, *, mu: ndarray, wt: ndarray, scale: ndarray) -&gt; ndarray\n</code></pre> <p>Gaussian CDF.</p>"},{"location":"api/families/#pymgcv.families.Gamma","title":"Gamma","text":"<pre><code>Gamma(link: Literal['inverse', 'identity', 'log'] = 'inverse')\n</code></pre> <p>Gamma family with specified link function.</p> <p>Parameters:</p> <ul> <li> <code>link</code>               (<code>Literal['inverse', 'identity', 'log']</code>, default:                   <code>'inverse'</code> )           \u2013            <p>The link function for the Gamma family.</p> </li> </ul>"},{"location":"api/families/#pymgcv.families.Gamma.cdf","title":"cdf","text":"<pre><code>cdf(x: ndarray, *, mu: ndarray, wt: ndarray, scale: ndarray)\n</code></pre> <p>Gamma CDF, wt is ignored.</p>"},{"location":"api/families/#pymgcv.families.InverseGaussian","title":"InverseGaussian","text":"<pre><code>InverseGaussian(link: Literal['1/mu^2', 'inverse', 'identity', 'log'] = '1/mu^2')\n</code></pre> <p>Inverse Gaussian family with specified link function.</p> <p>Parameters:</p> <ul> <li> <code>link</code>               (<code>Literal['1/mu^2', 'inverse', 'identity', 'log']</code>, default:                   <code>'1/mu^2'</code> )           \u2013            <p>The link function for the inverse Gaussian family.</p> </li> </ul>"},{"location":"api/families/#pymgcv.families.Tweedie","title":"Tweedie","text":"<pre><code>Tweedie(\n    p: float | int,\n    link: Union[Literal[\"log\", \"identity\", \"inverse\", \"sqrt\"], int, float] = 0,\n)\n</code></pre> <p>Tweedie family with fixed power.</p> <p>Parameters:</p> <ul> <li> <code>p</code>               (<code>float | int</code>)           \u2013            <p>The variance of an observation is proportional to its mean to the power p. p must be greater than 1 and less than or equal to 2. 1 would be Poisson, 2 is gamma.</p> </li> <li> <code>link</code>               (<code>Union[Literal['log', 'identity', 'inverse', 'sqrt'], int, float]</code>, default:                   <code>0</code> )           \u2013            <p>If a float/int, treated as \\(\\lambda\\) in a link function based on \\(\\eta = \\mu^\\lambda\\), meaning 0 gives the log link and 1 gives the identity link (i.e. R stats package <code>power</code>). Can also be one of \"log\", \"identity\", \"inverse\", \"sqrt\".</p> </li> </ul>"},{"location":"api/families/#pymgcv.families.Tw","title":"Tw","text":"<pre><code>Tw(\n    link: Literal[\"log\", \"identity\", \"inverse\", \"sqrt\"] = \"log\",\n    a: float = 1.01,\n    b: float = 1.99,\n    theta: float | int | None = None,\n    *,\n    theta_fixed: bool = False,\n)\n</code></pre> <p>Tweedie family with estimated power.</p> <p>Restricted to variance function powers between 1 and 2.</p> <p>Parameters:</p> <ul> <li> <code>link</code>               (<code>Literal['log', 'identity', 'inverse', 'sqrt']</code>, default:                   <code>'log'</code> )           \u2013            <p>The link function to use.</p> </li> <li> <code>a</code>               (<code>float</code>, default:                   <code>1.01</code> )           \u2013            <p>The lower bound of the power parameter for optimization.</p> </li> <li> <code>b</code>               (<code>float</code>, default:                   <code>1.99</code> )           \u2013            <p>The upper bound of the power parameter for optimization.</p> </li> <li> <code>theta</code>               (<code>float | int | None</code>, default:                   <code>None</code> )           \u2013            <p>Related to the Tweedie power parameter by \\(p=(a+b \\exp(\\theta))/(1+\\exp(\\theta))\\). If this is supplied as a positive value then it is taken as the fixed value for p. If it is a negative values then its absolute value is taken as the initial value for p.</p> </li> <li> <code>theta_fixed</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If theta is provided, controls whether to treat theta as fixed or estimated. If estimated, then theta is the starting value.</p> </li> </ul>"},{"location":"api/families/#pymgcv.families.Scat","title":"Scat","text":"<pre><code>Scat(\n    link: Literal[\"identity\", \"log\", \"inverse\"] = \"identity\",\n    min_df: float | int = 3,\n    theta: ndarray | None = None,\n    *,\n    theta_fixed: bool = False,\n)\n</code></pre> <p>Scaled t family for heavy tailed data.</p> <p>The idea is that \\((y-\\mu)/\\sigma \\sim t_\\nu\\) where \\(\\mu\\) is determined by a linear predictor, while \\(\\sigma\\) and \\(\\nu\\) are parameters to be estimated alongside the smoothing parameters.</p> <p>Parameters:</p> <ul> <li> <code>link</code>               (<code>Literal['identity', 'log', 'inverse']</code>, default:                   <code>'identity'</code> )           \u2013            <p>The link function to use.</p> </li> <li> <code>min_df</code>               (<code>float | int</code>, default:                   <code>3</code> )           \u2013            <p>The minimum degrees of freedom. Must be &gt;2 to avoid infinite response variance.</p> </li> <li> <code>theta</code>               (<code>ndarray | None</code>, default:                   <code>None</code> )           \u2013            <p>The parameters to be estimated \\(\\nu = b + \\exp(\\theta_1)\\) (where \\(b\\) is <code>min_df</code>) and \\(\\sigma = \\exp(\\theta_2)\\). If supplied and both positive, then taken to be fixed values of \\(\\nu\\) and \\(\\sigma\\). If any negative, then absolute values taken as starting values.</p> </li> <li> <code>theta_fixed</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If theta is provided, controls whether to treat theta as fixed or estimated. If estimated, then theta is the starting value.</p> </li> </ul>"},{"location":"api/families/#pymgcv.families.MVN","title":"MVN","text":"<pre><code>MVN(d: int)\n</code></pre> <p>Multivariate normal family.</p> <p>For this family, we expect \\(d\\) linear predictors for the means, each with a key corresponding to a variable name in data. The covariance is estimated during fitting. For this family, deviance residuals are standardized to be approximately indpendent standard normal.</p> <p>Parameters:</p> <ul> <li> <code>d</code>               (<code>int</code>)           \u2013            <p>The dimension of the distribution.</p> </li> </ul>"},{"location":"api/families/#count-and-proportions","title":"Count and proportions","text":""},{"location":"api/families/#pymgcv.families.Poisson","title":"Poisson","text":"<pre><code>Poisson(link: Literal['log', 'identity', 'sqrt'] = 'log')\n</code></pre> <p>Poisson family with specified link function.</p> <p>Parameters:</p> <ul> <li> <code>link</code>               (<code>Literal['log', 'identity', 'sqrt']</code>, default:                   <code>'log'</code> )           \u2013            <p>The link function for the Poisson family.</p> </li> </ul>"},{"location":"api/families/#pymgcv.families.Poisson.cdf","title":"cdf","text":"<pre><code>cdf(\n    x: ndarray, *, mu: ndarray, wt: ndarray | None = None, scale: ndarray | None = None\n)\n</code></pre> <p>Cumulative distribution function.</p>"},{"location":"api/families/#pymgcv.families.NegativeBinomial","title":"NegativeBinomial","text":"<pre><code>NegativeBinomial(\n    theta: float | int | None = None,\n    link: Literal[\"log\", \"identity\", \"sqrt\"] = \"log\",\n    *,\n    theta_fixed: bool = False,\n)\n</code></pre> <p>Negative binomial family.</p> <p>Parameters:</p> <ul> <li> <code>theta</code>               (<code>float | int | None</code>, default:                   <code>None</code> )           \u2013            <p>The positive parameter such that \\(\\text{var}(y) = \\mu + \\mu^2/\\theta\\), where \\(\\mu = \\mathbb{E}[y]\\).</p> </li> <li> <code>link</code>               (<code>Literal['log', 'identity', 'sqrt']</code>, default:                   <code>'log'</code> )           \u2013            <p>The link function to use.</p> </li> <li> <code>theta_fixed</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to treat theta as fixed or estimated. If estimated, then theta is the starting value.</p> </li> </ul>"},{"location":"api/families/#pymgcv.families.ZIP","title":"ZIP","text":"<pre><code>ZIP(b: int | float = 0, theta: tuple[int | float, int | float] | None = None)\n</code></pre> <p>Zero-inflated Poisson family.</p> <p>The probability of a zero count is given by \\(1-p\\), whereas the probability of count \\(y&gt;0\\) is given by the truncated Poisson probability function \\(p\\mu^y/((\\exp(\\mu)-1)y!)\\). The linear predictor gives \\(\\log \\mu\\), while \\(\\eta = \\log(-\\log(1-p))\\) and \\(\\eta = \\theta_1 + \\{b+\\exp(\\theta_2)\\} \\log \\mu\\). The theta parameters are estimated alongside the smoothing parameters. Increasing the b parameter from zero can greatly reduce identifiability problems, particularly when there are very few non-zero data.</p> <p>The fitted values for this model are the log of the Poisson parameter. Use the predict function with type==\"response\" to get the predicted expected response. Note that the theta parameters reported in model summaries are \\(\\theta_1 and b + \\exp(\\theta_2)\\).</p> <p>Warning</p> <p>These models should be subject to very careful checking, especially if fitting has not converged. It is quite easy to set up models with identifiability problems, particularly if the data are not really zero inflated, but simply have many zeroes because the mean is very low in some parts of the covariate space.</p> <p>Parameters:</p> <ul> <li> <code>b</code>               (<code>int | float</code>, default:                   <code>0</code> )           \u2013            <p>A non-negative constant, specifying the minimum dependence of the zero inflation rate on the linear predictor.</p> </li> <li> <code>theta</code>               (<code>tuple[int | float, int | float] | None</code>, default:                   <code>None</code> )           \u2013            <p>The 2 parameters controlling the slope and intercept of the linear transform of the mean controlling the zero inflation rate. If supplied then treated as fixed parameters (\\theta_1 and \\theta_2), otherwise estimated.</p> </li> </ul>"},{"location":"api/families/#pymgcv.families.Betar","title":"Betar","text":"<pre><code>Betar(\n    phi: float | int,\n    link: Literal[\"logit\", \"probit\", \"cauchit\", \"cloglog\"] = \"logit\",\n    eps: float = 1e-10,\n)\n</code></pre> <p>Beta regression family for use with GAM/BAM.</p> <p>The linear predictor controls the mean \\(\\mu\\), and the variance is given by \\(\\mu(1-\\mu)/(1+\\phi)\\). Note, any observations too close to zero or one will be clipped to <code>eps</code> and <code>1-eps</code> respsectively, to ensure the log likelihood is bounded for all parameter values.</p> <p>Parameters:</p> <ul> <li> <code>phi</code>               (<code>float | int</code>)           \u2013            <p>The parameter \\(\\phi\\), influencing the variance.</p> </li> <li> <code>link</code>               (<code>Literal['logit', 'probit', 'cauchit', 'cloglog']</code>, default:                   <code>'logit'</code> )           \u2013            <p>The link function to use.</p> </li> <li> <code>eps</code>               (<code>float</code>, default:                   <code>1e-10</code> )           \u2013            <p>Amount to clip values too close to zero or one.</p> </li> </ul>"},{"location":"api/families/#categorical-and-ordinal","title":"Categorical and ordinal","text":""},{"location":"api/families/#pymgcv.families.Binomial","title":"Binomial","text":"<pre><code>Binomial(link: Literal['logit', 'probit', 'cauchit', 'log', 'cloglog'] = 'logit')\n</code></pre> <p>Binomial family with specified link function.</p> <p>The response can be integers of zeros and ones (for binary data), proportions between zero and one (in which case the count can be incorporated as a weight), or a two-column matrix with the success and failure counts.</p> <p>Parameters:</p> <ul> <li> <code>link</code>               (<code>Literal['logit', 'probit', 'cauchit', 'log', 'cloglog']</code>, default:                   <code>'logit'</code> )           \u2013            <p>The link function. \"logit\", \"probit\" and \"cauchit\", correspond to logistic, normal and Cauchy CDFs respectively. \"cloglog\" is the complementary log-log.</p> </li> </ul>"},{"location":"api/families/#pymgcv.families.Binomial.cdf","title":"cdf","text":"<pre><code>cdf(x: ndarray, *, mu: ndarray, wt: ndarray, scale: ndarray)\n</code></pre> <p>Binomial CDF, scale is ignored.</p>"},{"location":"api/families/#pymgcv.families.OCat","title":"OCat","text":"<pre><code>OCat(num_categories: int)\n</code></pre> <p>Ordered categorical family.</p> <p>The response should be integer class labels, indexed from 1 (not a pandas ordered Categorical)!</p> <p>Parameters:</p> <ul> <li> <code>num_categories</code>               (<code>int</code>)           \u2013            <p>The number of categories.</p> </li> </ul>"},{"location":"api/families/#pymgcv.families.Multinom","title":"Multinom","text":"<pre><code>Multinom(k: int = 1)\n</code></pre> <p>Multinomial family.</p> <p>Categories must be coded as integers from 0 to K. This family can only be used with <code>GAM</code>. k predictors should be specified, with the first key matching the target variables name in the data. For the 0-th index, i.e. y=0, the likelihood is \\(1 / [1+\\sum_j \\exp(\\eta_j)\\), where \\(\\eta_j\\) is the j-th linear predictor. For y&gt;0, it is given by \\(\\exp(\\eta_{y})/(1+\\sum_j \\exp(\\eta_j))\\).</p> <p>Parameters:</p> <ul> <li> <code>k</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>There are k+1 categories, and k linear predictors.</p> </li> </ul>"},{"location":"api/families/#location-scale","title":"Location-scale","text":""},{"location":"api/families/#pymgcv.families.GauLSS","title":"GauLSS","text":"<pre><code>GauLSS(\n    link: Literal[\"identity\", \"inverse\", \"log\", \"sqrt\"] = \"identity\",\n    min_std: float = 0.01,\n)\n</code></pre> <p>Gaussian location-scale model family for GAMs.</p> <p>Models both the mean \\(\\mu\\) and standard deviation \\(\\sigma\\) of a Gaussian response. The standard deviation uses a \"logb\" link, i.e. \\(\\eta = \\log(\\sigma - b)\\) to avoid singularities near zero.</p> <p>Only compatible with <code>GAM</code>, to which two predictors must be specified, for the response variable and the scale respectively.</p> <ul> <li>Predictions with <code>type=\"response\"</code> returns columns <code>[mu, 1/sigma]</code></li> <li>Predictions with <code>type=\"link\"</code> returns columns <code>[eta_mu, log(sigma - b)]</code></li> <li>Plots use the <code>log(sigma - b)</code> scale.</li> </ul> <p>Parameters:</p> <ul> <li> <code>link</code>               (<code>Literal['identity', 'inverse', 'log', 'sqrt']</code>, default:                   <code>'identity'</code> )           \u2013            <p>The link function to use for \\(\\mu\\).</p> </li> <li> <code>min_std</code>               (<code>float</code>, default:                   <code>0.01</code> )           \u2013            <p>Minimum standard deviation \\(b\\), for the \"logb\" link.</p> </li> </ul>"},{"location":"api/families/#pymgcv.families.GammaLS","title":"GammaLS","text":"<pre><code>GammaLS(min_log_scale: float | int = -7)\n</code></pre> <p>Gamma location-scale model family.</p> <p>The log of the mean, \\(\\mu\\), and the log of the scale parameter, \\(\\phi\\) can depend on additive smooth predictors (i.e. using two formulae).</p> <p>Parameters:</p> <ul> <li> <code>min_log_scale</code>               (<code>float | int</code>, default:                   <code>-7</code> )           \u2013            <p>The minimum value for the log scales parameter.</p> </li> </ul>"},{"location":"api/families/#pymgcv.families.GevLSS","title":"GevLSS","text":"<pre><code>GevLSS(\n    location_link: Literal[\"identity\", \"log\"] = \"identity\",\n    shape_link: Literal[\"identity\", \"logit\"] = \"logit\",\n)\n</code></pre> <p>Generalized extreme value location, scale and shape family.</p> <p>Requires three predictors, one for the location, log scale and the shape.</p> <p>Uses the p.d.f. \\(t(y)^{\\xi+1} e^{-t(y)} / \\sigma\\), where: \\(t(x) = [1 + \\xi(y-\\mu)/\\sigma]^{-1/\\xi}\\) if \\(\\xi \\neq 0\\) and \\(\\exp[-(y-\\mu)/\\sigma]\\) otherwise.</p> <p>Parameters:</p> <ul> <li> <code>location_link</code>               (<code>Literal['identity', 'log']</code>, default:                   <code>'identity'</code> )           \u2013            <p>The link function to use for \\(\\mu\\).</p> </li> <li> <code>shape_link</code>               (<code>Literal['identity', 'logit']</code>, default:                   <code>'logit'</code> )           \u2013            <p>The link function to use for \\(\\xi\\).</p> </li> </ul>"},{"location":"api/families/#pymgcv.families.GumbLS","title":"GumbLS","text":"<pre><code>GumbLS(scale_link: Literal['identity', 'log'] = 'log', min_log_scale: float = -7)\n</code></pre> <p>Gumbel location scale additive model.</p> <p><code>gumbls</code> fits Gumbel location\u2013scale models with a location parameter \\(\\mu\\) and a log scale parameter \\(\\beta\\).</p> <p>For \\(z = (y - \\mu) e^{-\\beta}\\), the log Gumbel density is \\(\\ell = -\\beta - z - e^{-z}\\). The mean is \\(\\mu + \\gamma e^{\\beta}\\), and the variance is \\(\\pi^2 e^{2\\beta}/6\\).</p> <p>Note predictions on the response scale will return the log scale \\(\\beta\\)</p> <p>Warning</p> <p>Read the documentation for the <code>scale_link</code> parameter, which is potentially confusing (inherited from mgcv).</p> <p>Parameters:</p> <ul> <li> <code>scale_link</code>               (<code>Literal['identity', 'log']</code>, default:                   <code>'log'</code> )           \u2013            <p>The link for the log scale parameter \\(\\beta\\), defined as followed:</p> <ul> <li><code>scale_link=\"identity\"</code>: linear predictor directly gives \u03b2.</li> <li><code>scale_link=\"log\"</code>: ensures \\(\\beta &gt; b\\) using     \\(\\beta = b + log(1 + exp(\u03b7))\\).</li> </ul> </li> <li> <code>min_log_scale</code>               (<code>float</code>, default:                   <code>-7</code> )           \u2013            <p>The minimum value for the log scale parameter (<code>b</code> above) if using the log link.</p> </li> </ul>"},{"location":"api/families/#pymgcv.families.Shash","title":"Shash","text":"<pre><code>Shash(b: float = 0.01, phi_pen: float = 0.001)\n</code></pre> <p>Sinh-arcsinh location scale and shape model family.</p> <p>Implements the four-parameter sinh-arcsinh (shash) distribution of Jones and Pewsey (2009). The location, scale, skewness and kurtosis of the density can depend on additive smooth predictors. Requires four <code>predictors</code>, with the first (the location), corresponding to a variable name in the data, and the rest denoting the scale, skewness and kurtosis (for which any names can be chosen).</p> <p>The density function is: $$ p(y|\\mu,\\sigma,\\epsilon,\\delta)=C(z) \\exp{-S(z)^2/2} {2\\pi(1+z^2)}^{-1/2}/\\sigma $$</p> <p>where \\(C(z) = \\{1+S(z)^2\\}^{1/2}\\), \\(S(z) = \\sinh\\{\\delta \\sinh^{-1}(z) - \\epsilon\\}\\) and \\(z = (y - \\mu)/(\\sigma \\delta)\\). \\(\\mu\\) controls the location, \\(\\sigma\\) controls the scale, \\(\\epsilon\\) controls the skewness, and \\(\\delta\\) the tail weight. For fitting purposes, we use \\(\\tau = \\log(\\sigma)\\) and \\(\\phi = \\log(\\delta)\\).</p> <p>The link functions are fixed at identity for all parameters except the scale \\(\\tau\\), which uses logeb, defined as \\(\\eta = \\log [\\exp(\\tau) - b]\\), such that the inverse is \\(\\tau = \\log(\\sigma) = \\log\\{\\exp(\\eta)+b\\}\\).</p> <p>Parameters:</p> <ul> <li> <code>b</code>               (<code>float</code>, default:                   <code>0.01</code> )           \u2013            <p>Positive parameter for the minimum scale of the logeb link function for the scale parameter.</p> </li> <li> <code>phi_pen</code>               (<code>float</code>, default:                   <code>0.001</code> )           \u2013            <p>Positive multiplier of a ridge penalty on kurtosis parameter, shrinking towards zero.</p> </li> </ul>"},{"location":"api/families/#quasi-likelihood","title":"Quasi-likelihood","text":""},{"location":"api/families/#pymgcv.families.Quasi","title":"Quasi","text":"<pre><code>Quasi(\n    link: Literal[\n        \"logit\", \"probit\", \"cloglog\", \"identity\", \"inverse\", \"log\", \"1/mu^2\", \"sqrt\"\n    ] = \"identity\",\n    variance: Literal[\"constant\", \"mu(1-mu)\", \"mu\", \"mu^2\", \"mu^3\"] = \"constant\",\n)\n</code></pre> <p>Quasi family with specified link and variance functions.</p> <p>Parameters:</p> <ul> <li> <code>link</code>               (<code>Literal['logit', 'probit', 'cloglog', 'identity', 'inverse', 'log', '1/mu^2', 'sqrt']</code>, default:                   <code>'identity'</code> )           \u2013            <p>The link function for the quasi family.</p> </li> <li> <code>variance</code>               (<code>Literal['constant', 'mu(1-mu)', 'mu', 'mu^2', 'mu^3']</code>, default:                   <code>'constant'</code> )           \u2013            <p>The variance function for the quasi family.</p> </li> </ul>"},{"location":"api/families/#pymgcv.families.QuasiBinomial","title":"QuasiBinomial","text":"<pre><code>QuasiBinomial(link: Literal['logit', 'probit', 'cauchit', 'log', 'cloglog'] = 'logit')\n</code></pre> <p>Quasi-binomial family with specified link function.</p> <p>Parameters:</p> <ul> <li> <code>link</code>               (<code>Literal['logit', 'probit', 'cauchit', 'log', 'cloglog']</code>, default:                   <code>'logit'</code> )           \u2013            <p>The link function for the quasi-binomial family.</p> </li> </ul>"},{"location":"api/families/#pymgcv.families.QuasiPoisson","title":"QuasiPoisson","text":"<pre><code>QuasiPoisson(link: Literal['log', 'identity', 'sqrt'] = 'log')\n</code></pre> <p>Quasi-Poisson family with specified link function.</p> <p>Parameters:</p> <ul> <li> <code>link</code>               (<code>Literal['log', 'identity', 'sqrt']</code>, default:                   <code>'log'</code> )           \u2013            <p>The link function for the quasi-Poisson family.</p> </li> </ul>"},{"location":"api/families/#not-yet-implemented","title":"Not yet implemented","text":""},{"location":"api/families/#pymgcv.families.TwLSS","title":"TwLSS","text":"<pre><code>TwLSS()\n</code></pre> <p>Not yet implemented.</p>"},{"location":"api/families/#pymgcv.families.ZipLSS","title":"ZipLSS","text":"<pre><code>ZipLSS()\n</code></pre> <p>Not yet implemented.</p>"},{"location":"api/families/#pymgcv.families.CNorm","title":"CNorm","text":"<pre><code>CNorm()\n</code></pre> <p>Not yet implemented.</p>"},{"location":"api/families/#pymgcv.families.CLog","title":"CLog","text":"<pre><code>CLog()\n</code></pre> <p>Not yet implemented.</p>"},{"location":"api/families/#pymgcv.families.CPois","title":"CPois","text":"<pre><code>CPois()\n</code></pre> <p>Not yet implemented.</p>"},{"location":"api/families/#pymgcv.families.CoxPH","title":"CoxPH","text":"<pre><code>CoxPH()\n</code></pre> <p>Not yet implmented.</p> <p>Additive Cox Proportional Hazard Model.</p> <p>Cox Proportional Hazards model with Peto's correction for ties, optional stratification, and estimation by penalized partial likelihood maximization, for use with <code>GAM</code>. In the model formula, event time is the response.</p> <p>Under stratification the response has two columns: time and a numeric index for stratum. The weights vector provides the censoring information (0 for censoring, 1 for event). CoxPH deals with the case in which each subject has one event/censoring time and one row of covariate values.</p>"},{"location":"api/families/#base-classes","title":"Base classes","text":""},{"location":"api/families/#pymgcv.families.AbstractFamily","title":"AbstractFamily","text":"<p>Provides default implmentations for distribution methods.</p> <p>This applies mgcv <code>fix.family.qf</code> for the quantile function, and <code>fix.family.rd</code> for the sampling function.</p>"},{"location":"api/families/#pymgcv.families.AbstractFamily.n_predictors","title":"n_predictors  <code>property</code>","text":"<pre><code>n_predictors\n</code></pre> <p>Return the total number of predictors.</p>"},{"location":"api/families/#pymgcv.families.AbstractFamily.link","title":"link","text":"<pre><code>link(x: ndarray) -&gt; ndarray\n</code></pre> <p>Compute the link function.</p>"},{"location":"api/families/#pymgcv.families.AbstractFamily.inverse_link","title":"inverse_link","text":"<pre><code>inverse_link(x: ndarray) -&gt; ndarray\n</code></pre> <p>Compute the inverse link function.</p>"},{"location":"api/families/#pymgcv.families.AbstractFamily.dmu_deta","title":"dmu_deta","text":"<pre><code>dmu_deta(x: ndarray) -&gt; ndarray\n</code></pre> <p>Compute the derivative dmu/deta of the link function.</p>"},{"location":"api/families/#pymgcv.families.AbstractFamily.sample","title":"sample","text":"<pre><code>sample(\n    mu: int | float | ndarray,\n    wt: int | float | ndarray | None = None,\n    scale: int | float | ndarray | None = None,\n)\n</code></pre> <p>Sample the family distributions (R family rd method).</p>"},{"location":"api/families/#pymgcv.families.SupportsCDF","title":"SupportsCDF","text":"<p>Mixin for families supporting cumulative distribution functions.</p>"},{"location":"api/families/#pymgcv.families.SupportsCDF.cdf","title":"cdf","text":"<pre><code>cdf(x: ndarray, *, mu: ndarray, wt: ndarray, scale: ndarray) -&gt; ndarray\n</code></pre> <p>Cumulative distribution function.</p>"},{"location":"api/gam/","title":"GAMs","text":"<p>GAM model types from <code>pymgcv.gam</code>.</p>"},{"location":"api/gam/#pymgcv.gam.GAM","title":"GAM","text":"<pre><code>GAM(\n    predictors: Mapping[str, Iterable[AbstractTerm] | AbstractTerm],\n    *,\n    family: AbstractFamily | None = None,\n    add_intercepts: bool = True,\n)\n</code></pre> <p>Standard GAM Model.</p> <p>Initialize the model.</p> <p>Parameters:</p> <ul> <li> <code>predictors</code>               (<code>Mapping[str, Iterable[AbstractTerm] | AbstractTerm]</code>)           \u2013            <p>Dictionary mapping target variable names to an iterable of <code>AbstractTerm</code> objects used to predict \\(g([\\mathbb{E}[Y])\\).</p> <ul> <li>For simple models, this will usually be a single     key-value pair:     <pre><code>{\"y\": S(\"x1\") + S(\"x2\")}\n</code></pre></li> <li>For multivariate models, e.g. <code>MVN</code>, the     dictionary will have multiple pairs:         <pre><code>{\"y1\": S(\"x1\") + S(\"x2\"), \"y2\": S(\"x2\")}\n</code></pre></li> <li>For multiparameter models, such as LSS-type models (e.g. <code>GauLSS</code>), the first key-value pair must correspond to the variable name in the data (usually modelling the location), and the subsequent dictionary elements model the other parameters in the order as defined by the family (e.g. scale and shape). The names of these extra parameters, can be anything, and are used as column names for prediction outputs. <pre><code>{\"y\": S(\"x1\") + S(\"x2\"), \"scale\": S(\"x2\")}\n</code></pre></li> </ul> </li> <li> <code>family</code>               (<code>AbstractFamily | None</code>, default:                   <code>None</code> )           \u2013            <p>Distribution family to use. See Families for available options.</p> </li> <li> <code>add_intercepts</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If False, intercept terms must be manually added to the formulae using <code>Intercept</code>. If True, automatically adds an intercept term to each formula. Intercepts are added as needed by methods, such that <code>gam.predictors</code> reflect the model as constructed (i.e. before adding intercepts).</p> </li> </ul>"},{"location":"api/gam/#pymgcv.gam.GAM.fit","title":"fit","text":"<pre><code>fit(\n    data: DataFrame | Mapping[str, ndarray | Series],\n    *,\n    method: Literal[\n        \"GCV.Cp\", \"GACV.Cp\", \"QNCV\", \"REML\", \"P-REML\", \"ML\", \"P-ML\", \"NCV\"\n    ] = \"REML\",\n    weights: str | ndarray | Series | None = None,\n    optimizer: str | tuple[str, str] = (\"outer\", \"newton\"),\n    scale: Union[Literal[\"unknown\"], float, int, NoneType] = None,\n    select: bool = False,\n    gamma: float | int = 1,\n    knots: dict[str, ndarray] | None = None,\n    n_threads: int = 1,\n) -&gt; typing.Self\n</code></pre> <p>Fit the GAM.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>DataFrame | Mapping[str, ndarray | Series]</code>)           \u2013            <p>DataFrame or dictionary containing all variables referenced in the model. Note, using a dictionary is required when passing matrix-valued variables.</p> </li> <li> <code>method</code>               (<code>Literal['GCV.Cp', 'GACV.Cp', 'QNCV', 'REML', 'P-REML', 'ML', 'P-ML', 'NCV']</code>, default:                   <code>'REML'</code> )           \u2013            <p>Method for smoothing parameter estimation, matching the mgcv options.</p> </li> <li> <code>weights</code>               (<code>str | ndarray | Series | None</code>, default:                   <code>None</code> )           \u2013            <p>Observation weights. Either a string, matching a column name, or an array/series with length equal to the number of observations.</p> </li> <li> <code>optimizer</code>               (<code>str | tuple[str, str]</code>, default:                   <code>('outer', 'newton')</code> )           \u2013            <p>An string or length 2 tuple, specifying the numerical optimization method to use to optimize the smoothing parameter estimation criterion (given by method). \"outer\" for the direct nested optimization approach. \"outer\" can use several alternative optimizers, specified in the second element: \"newton\" (default), \"bfgs\", \"optim\" or \"nlm\". \"efs\" for the extended Fellner Schall method of Wood and Fasiolo (2017).</p> </li> <li> <code>scale</code>               (<code>Union[Literal['unknown'], float, int, NoneType]</code>, default:                   <code>None</code> )           \u2013            <p>If a number is provided, it is treated as a known scale parameter. If left to None, the scale parameter is 1 for Poisson and binomial and unknown otherwise. Note that (RE)ML methods can only work with scale parameter 1 for the Poisson and binomial cases.</p> </li> <li> <code>select</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If set to True then gam can add an extra penalty to each term so that it can be penalized to zero. This means that the smoothing parameter estimation during fitting can completely remove terms from the model. If the corresponding smoothing parameter is estimated as zero then the extra penalty has no effect. Use gamma to increase level of penalization.</p> </li> <li> <code>gamma</code>               (<code>float | int</code>, default:                   <code>1</code> )           \u2013            <p>Increase this beyond 1 to produce smoother models. gamma multiplies the effective degrees of freedom in the GCV or UBRE/AIC. gamma can be viewed as an effective sample size in the GCV score, and this also enables it to be used with REML/ML. Ignored with P-RE/ML or the efs optimizer.</p> </li> <li> <code>knots</code>               (<code>dict[str, ndarray] | None</code>, default:                   <code>None</code> )           \u2013            <p>Dictionary mapping covariate names to knot locations. For most bases, the length of the knot locations should match with a user supplied <code>k</code> value. E.g. for <code>S(\"x\", k=64)</code>, you could pass <code>knots={\"x\": np.linspace(0, 1, 64)}</code>. For multidimensional smooths, e.g. <code>S(\"x\", \"z\", k=64)</code>, you could create a grid of coordinates:</p> <p>Example</p> <pre><code>    import numpy as np\n    coords = np.linspace(0, 1, num=8)\n    X, Z = np.meshgrid(coords, coords)\n    knots = {\"x\": X.ravel(), \"z\": Z.ravel()}\n</code></pre> <p>Note if using <code>ThinPlateSpline</code>, this will avoid the eigen-decomposition used to find the basis, which although fast often leads to worse results. Different terms can use different numbers of knots, unless they share covariates.</p> </li> <li> <code>n_threads</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of threads to use for fitting the GAM.</p> </li> </ul>"},{"location":"api/gam/#pymgcv.gam.GAM.predict","title":"predict","text":"<pre><code>predict(\n    data: DataFrame | Mapping[str, ndarray | Series] | None = None,\n    *,\n    compute_se: bool = False,\n    type: Literal[\"response\", \"link\"] = \"link\",\n    block_size: int | None = None,\n) -&gt; dict[str, numpy.ndarray] | dict[str, pymgcv.custom_types.FitAndSE[numpy.ndarray]]\n</code></pre> <p>Compute model predictions with (optionally) uncertainty estimates.</p> <p>Makes predictions for new data using the fitted GAM model. Predictions are returned on the link scale (linear predictor scale), not the response scale. For response scale predictions, apply the appropriate inverse link function to the results.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>DataFrame | Mapping[str, ndarray | Series] | None</code>, default:                   <code>None</code> )           \u2013            <p>A dictionary or DataFrame containing all variables referenced in the model. Defaults to the data used to fit the model.</p> </li> <li> <code>compute_se</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to compute standard errors for predictions.</p> </li> <li> <code>type</code>               (<code>Literal['response', 'link']</code>, default:                   <code>'link'</code> )           \u2013            <p>Type of prediction to compute. Either \"link\" for linear predictor scale or \"response\" for response scale.</p> </li> <li> <code>block_size</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Number of rows to process at a time.  If None then block size is 1000 if data supplied, and the number of rows in the model frame otherwise.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, ndarray] | dict[str, FitAndSE[ndarray]]</code>           \u2013            <p>A dictionary mapping the target variable names to a pandas DataFrame</p> </li> <li> <code>dict[str, ndarray] | dict[str, FitAndSE[ndarray]]</code>           \u2013            <p>containing the predictions and standard errors if <code>se</code> is True.</p> </li> </ul>"},{"location":"api/gam/#pymgcv.gam.GAM.partial_effects","title":"partial_effects","text":"<pre><code>partial_effects(\n    data: DataFrame | Mapping[str, Series | ndarray] | None = None,\n    *,\n    compute_se: bool = False,\n    block_size: int | None = None,\n) -&gt; (\n    dict[str, pandas.core.frame.DataFrame]\n    | dict[str, pymgcv.custom_types.FitAndSE[pandas.core.frame.DataFrame]]\n)\n</code></pre> <p>Compute partial effects for all model terms.</p> <p>Calculates the contribution of each model term to the overall prediction on the link scale. The sum of all fit columns equals the total prediction (link scale).</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>DataFrame | Mapping[str, Series | ndarray] | None</code>, default:                   <code>None</code> )           \u2013            <p>A dictionary or DataFrame containing all variables referenced in the model. Defaults to the data used to fit the model.</p> </li> <li> <code>compute_se</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to compute and return standard errors.</p> </li> <li> <code>block_size</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Number of rows to process at a time.  If None then block size is 1000 if data supplied, and the number of rows in the model frame otherwise.</p> </li> </ul>"},{"location":"api/gam/#pymgcv.gam.BAM","title":"BAM","text":"<pre><code>BAM(\n    predictors: Mapping[str, Iterable[AbstractTerm] | AbstractTerm],\n    *,\n    family: AbstractFamily | None = None,\n    add_intercepts: bool = True,\n)\n</code></pre> <p>A big-data GAM (BAM) model.</p> <p>Initialize the model.</p> <p>Parameters:</p> <ul> <li> <code>predictors</code>               (<code>Mapping[str, Iterable[AbstractTerm] | AbstractTerm]</code>)           \u2013            <p>Dictionary mapping target variable names to an iterable of <code>AbstractTerm</code> objects used to predict \\(g([\\mathbb{E}[Y])\\).</p> <ul> <li>For simple models, this will usually be a single     key-value pair:     <pre><code>{\"y\": S(\"x1\") + S(\"x2\")}\n</code></pre></li> <li>For multivariate models, e.g. <code>MVN</code>, the     dictionary will have multiple pairs:         <pre><code>{\"y1\": S(\"x1\") + S(\"x2\"), \"y2\": S(\"x2\")}\n</code></pre></li> <li>For multiparameter models, such as LSS-type models (e.g. <code>GauLSS</code>), the first key-value pair must correspond to the variable name in the data (usually modelling the location), and the subsequent dictionary elements model the other parameters in the order as defined by the family (e.g. scale and shape). The names of these extra parameters, can be anything, and are used as column names for prediction outputs. <pre><code>{\"y\": S(\"x1\") + S(\"x2\"), \"scale\": S(\"x2\")}\n</code></pre></li> </ul> </li> <li> <code>family</code>               (<code>AbstractFamily | None</code>, default:                   <code>None</code> )           \u2013            <p>Distribution family to use. See Families for available options.</p> </li> <li> <code>add_intercepts</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If False, intercept terms must be manually added to the formulae using <code>Intercept</code>. If True, automatically adds an intercept term to each formula. Intercepts are added as needed by methods, such that <code>gam.predictors</code> reflect the model as constructed (i.e. before adding intercepts).</p> </li> </ul>"},{"location":"api/gam/#pymgcv.gam.BAM.fit","title":"fit","text":"<pre><code>fit(\n    data: DataFrame | Mapping[str, ndarray | Series],\n    *,\n    method: Literal[\n        \"fREML\", \"GCV.Cp\", \"GACV.Cp\", \"REML\", \"P-REML\", \"ML\", \"P-ML\", \"NCV\"\n    ] = \"fREML\",\n    weights: str | ndarray | Series | None = None,\n    scale: Union[Literal[\"unknown\"], float, int, NoneType] = None,\n    select: bool = False,\n    gamma: float | int = 1,\n    knots: dict[str, ndarray] | None = None,\n    chunk_size: int = 10000,\n    discrete: bool = False,\n    samfrac: float | int = 1,\n    n_threads: int = 1,\n    gc_level: Literal[0, 1, 2] = 0,\n) -&gt; typing.Self\n</code></pre> <p>Fit the GAM.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>DataFrame | Mapping[str, ndarray | Series]</code>)           \u2013            <p>DataFrame or dictionary containing all variables referenced in the model. Note, using a dictionary is required when passing matrix-valued variables.</p> </li> <li> <code>method</code>               (<code>Literal['fREML', 'GCV.Cp', 'GACV.Cp', 'REML', 'P-REML', 'ML', 'P-ML', 'NCV']</code>, default:                   <code>'fREML'</code> )           \u2013            <p>Method for smoothing parameter estimation, matching the mgcv, options.</p> </li> <li> <code>weights</code>               (<code>str | ndarray | Series | None</code>, default:                   <code>None</code> )           \u2013            <p>Observation weights. Either a string, matching a column name, or a array/series with length equal to the number of observations.</p> </li> <li> <code>scale</code>               (<code>Union[Literal['unknown'], float, int, NoneType]</code>, default:                   <code>None</code> )           \u2013            <p>If a number is provided, it is treated as a known scale parameter. If left to None, the scale parameter is 1 for Poisson and binomial and unknown otherwise. Note that (RE)ML methods can only work with scale parameter 1 for the Poisson and binomial cases.</p> </li> <li> <code>select</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If set to True then gam can add an extra penalty to each term so that it can be penalized to zero. This means that the smoothing parameter estimation during fitting can completely remove terms from the model. If the corresponding smoothing parameter is estimated as zero then the extra penalty has no effect. Use gamma to increase level of penalization.</p> </li> <li> <code>gamma</code>               (<code>float | int</code>, default:                   <code>1</code> )           \u2013            <p>Increase this beyond 1 to produce smoother models. gamma multiplies the effective degrees of freedom in the GCV or UBRE/AIC. gamma can be viewed as an effective sample size in the GCV score, and this also enables it to be used with REML/ML. Ignored with P-RE/ML or the efs optimizer.</p> </li> <li> <code>knots</code>               (<code>dict[str, ndarray] | None</code>, default:                   <code>None</code> )           \u2013            <p>Dictionary mapping covariate names to knot locations. For most bases, the length of the knot locations should match with a user supplied <code>k</code> value. E.g. for <code>S(\"x\", k=64)</code>, you could pass <code>knots={\"x\": np.linspace(0, 1, 64)}</code>. For multidimensional smooths, e.g. <code>S(\"x\", \"z\", k=64)</code>, you could create a grid of coordinates:</p> <p>Example</p> <pre><code>    import numpy as np\n    coords = np.linspace(0, 1, num=8)\n    X, Z = np.meshgrid(coords, coords)\n    knots = {\"x\": X.ravel(), \"z\": Z.ravel()}\n</code></pre> <p>Note if using <code>ThinPlateSpline</code>, this will avoid the eigen-decomposition used to find the basis, which although fast often leads to worse results. Different terms can use different numbers of knots, unless they share covariates.</p> </li> <li> <code>chunk_size</code>               (<code>int</code>, default:                   <code>10000</code> )           \u2013            <p>The model matrix is created in chunks of this size, rather than ever being formed whole. Reset to 4p if chunk.size &lt; 4p where p is the number of coefficients.</p> </li> <li> <code>discrete</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>if True and using method=\"fREML\", discretizes covariates for storage and efficiency reasons.</p> </li> <li> <code>samfrac</code>               (<code>float | int</code>, default:                   <code>1</code> )           \u2013            <p>If <code>0&lt;samfrac&lt;1</code>, performs a fast preliminary fitting step using a subsample of the data to improve convergence speed.</p> </li> <li> <code>n_threads</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of threads to use for fitting the GAM.</p> </li> <li> <code>gc_level</code>               (<code>Literal[0, 1, 2]</code>, default:                   <code>0</code> )           \u2013            <p>0 uses R's garbage collector, 1 and 2 use progressively more frequent garbage collection, which takes time but reduces memory requirements.</p> </li> </ul>"},{"location":"api/gam/#pymgcv.gam.BAM.predict","title":"predict","text":"<pre><code>predict(\n    data: DataFrame | Mapping[str, ndarray | Series] | None = None,\n    *,\n    compute_se: bool = False,\n    type: Literal[\"link\", \"response\"] = \"link\",\n    block_size: int = 50000,\n    discrete: bool = True,\n    n_threads: int = 1,\n    gc_level: Literal[0, 1, 2] = 0,\n) -&gt; dict[str, pymgcv.custom_types.FitAndSE[numpy.ndarray]] | dict[str, numpy.ndarray]\n</code></pre> <p>Compute model predictions with uncertainty estimates.</p> <p>Makes predictions for new data using the fitted GAM model. Predictions are returned on the link scale (linear predictor scale), not the response scale. For response scale predictions, apply the appropriate inverse link function to the results.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>DataFrame | Mapping[str, ndarray | Series] | None</code>, default:                   <code>None</code> )           \u2013            <p>A dictionary or DataFrame containing all variables referenced in the model. Defaults to the data used to fit the model.</p> </li> <li> <code>compute_se</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to compute and return standard errors.</p> </li> <li> <code>type</code>               (<code>Literal['link', 'response']</code>, default:                   <code>'link'</code> )           \u2013            <p>Type of prediction to compute. Either \"link\" for linear predictor scale or \"response\" for response scale.</p> </li> <li> <code>block_size</code>               (<code>int</code>, default:                   <code>50000</code> )           \u2013            <p>Number of rows to process at a time.</p> </li> <li> <code>n_threads</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of threads to use for computation.</p> </li> <li> <code>discrete</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True and the model was fitted with discrete=True, then uses discrete prediction methods in which covariates are discretized for efficiency for storage and efficiency reasons.</p> </li> <li> <code>gc_level</code>               (<code>Literal[0, 1, 2]</code>, default:                   <code>0</code> )           \u2013            <p>0 uses R's garbage collector, 1 and 2 use progressively more frequent garbage collection, which takes time but reduces memory requirements.</p> </li> </ul>"},{"location":"api/gam/#pymgcv.gam.BAM.partial_effects","title":"partial_effects","text":"<pre><code>partial_effects(\n    data: DataFrame | Mapping[str, ndarray | Series] | None = None,\n    *,\n    compute_se: bool = False,\n    block_size: int = 50000,\n    n_threads: int = 1,\n    discrete: bool = True,\n    gc_level: Literal[0, 1, 2] = 0,\n) -&gt; (\n    dict[str, pandas.core.frame.DataFrame]\n    | dict[str, pymgcv.custom_types.FitAndSE[pandas.core.frame.DataFrame]]\n)\n</code></pre> <p>Compute partial effects for all model terms.</p> <p>Calculates the contribution of each model term to the overall prediction. This decomposition is useful for understanding which terms contribute most to predictions and for creating partial effect plots. The sum of all fit columns equals the total prediction.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>DataFrame | Mapping[str, ndarray | Series] | None</code>, default:                   <code>None</code> )           \u2013            <p>A dictionary or DataFrame containing all variables referenced in the model. Defaults to the data used to fit the model.</p> </li> <li> <code>compute_se</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to compute and return standard errors.</p> </li> <li> <code>block_size</code>               (<code>int</code>, default:                   <code>50000</code> )           \u2013            <p>Number of rows to process at a time. Higher is faster but more memory intensive.</p> </li> <li> <code>n_threads</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of threads to use for computation.</p> </li> <li> <code>discrete</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True and the model was fitted with discrete=True, then uses discrete prediction methods in which covariates are discretized for efficiency for storage and efficiency reasons.</p> </li> <li> <code>gc_level</code>               (<code>Literal[0, 1, 2]</code>, default:                   <code>0</code> )           \u2013            <p>0 uses R's garbage collector, 1 and 2 use progressively more frequent garbage collection, which takes time but reduces memory requirements.</p> </li> </ul>"},{"location":"api/gam/#pymgcv.gam.AbstractGAM","title":"AbstractGAM","text":"<p>Abstract base class for GAM models.</p> <p>This class cannot be initialized but provides a common interface for fitting and predicting using different types of GAM models.</p>"},{"location":"api/gam/#pymgcv.gam.AbstractGAM.referenced_variables","title":"referenced_variables  <code>property</code>","text":"<pre><code>referenced_variables\n</code></pre> <p>List of variables referenced by the model required to be present in data.</p>"},{"location":"api/gam/#pymgcv.gam.AbstractGAM.summary","title":"summary","text":"<pre><code>summary() -&gt; str\n</code></pre> <p>Generate an mgcv-style summary of the fitted GAM model.</p>"},{"location":"api/gam/#pymgcv.gam.AbstractGAM.check_k","title":"check_k","text":"<pre><code>check_k(subsample: int = 5000, n_rep: int = 400) -&gt; DataFrame\n</code></pre> <p>Checking basis dimension choices (k).</p> <p>The default choices for <code>k</code> are relatively arbitrary. This function aids in assessing whether the chosen basis dimensions are appropriate. A low p-value can indicate that the chosen basis dimension is too low.</p> <p>The function works by constrasting a residual variance estimate based on near neighbour points (based on the covariates of a term), to the overall residual variance. The <code>k_index</code> is the ratio of the near neighbour estimate to the overall variance. The further below 1 the <code>k_index</code> is, the more likely it is that there exists missed patterns in the residuals. The p-value is generated using a randomization test to obtain the null distribution.</p> <p>For details, see section 5.9 of:</p> <pre><code>Wood S.N. (2017) Generalized Additive Models: An Introduction with R (2nd\nedition). Chapman and Hall/CRC Press.\n</code></pre> <p>Parameters:</p> <ul> <li> <code>subsample</code>               (<code>int</code>, default:                   <code>5000</code> )           \u2013            <p>The maximum number of points to use, above which a random subsample is used.</p> </li> <li> <code>n_rep</code>               (<code>int</code>, default:                   <code>400</code> )           \u2013            <p>The number of re-shuffles to do to get the p-value.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>A dataframe with the following columns:</p> <ul> <li><code>term</code>: The mgcv-style name of the smooth term.</li> <li><code>max_edf</code>: The maximum possible edf (often <code>k-1</code>).</li> <li><code>k_index</code>: The ratio between the nearest neighbour variance    residual variance estimate and the overall variance.</li> <li><code>p_value</code>: The p-value of the randomization test.</li> <li><code>max_edf</code>: The maximum effective degrees of freedom.</li> </ul> </li> </ul>"},{"location":"api/gam/#pymgcv.gam.AbstractGAM.coefficients","title":"coefficients","text":"<pre><code>coefficients() -&gt; Series\n</code></pre> <p>Extract model coefficients from the fitted GAM.</p> <p>Returns a series where the index if the mgcv-style name of the parameter.</p>"},{"location":"api/gam/#pymgcv.gam.AbstractGAM.covariance","title":"covariance","text":"<pre><code>covariance(\n    *, sandwich: bool = False, freq: bool = False, unconditional: bool = False\n) -&gt; DataFrame\n</code></pre> <p>Extract the covariance matrix from the fitted GAM.</p> <p>Extracts the Bayesian posterior covariance matrix of the parameters or frequentist covariance matrix of the parameter estimators from the fitted GAM.</p> <p>Parameters:</p> <ul> <li> <code>sandwich</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, compute sandwich estimate of covariance matrix. Currently expensive for discrete bam fits.</p> </li> <li> <code>freq</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, return the frequentist covariance matrix of the parameter estimators. If False, return the Bayesian posterior covariance matrix of the parameters. The latter option includes the expected squared bias according to the Bayesian smoothing prior.</p> </li> <li> <code>unconditional</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True (and freq=False), return the Bayesian smoothing parameter uncertainty corrected covariance matrix, if available.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>The covariance matrix as a pandas dataframe where the column names and index</p> </li> <li> <code>DataFrame</code>           \u2013            <p>are the mgcv-style parameter names.</p> </li> </ul>"},{"location":"api/gam/#pymgcv.gam.AbstractGAM.partial_effect","title":"partial_effect","text":"<pre><code>partial_effect(\n    term: AbstractTerm | int,\n    target: str | None = None,\n    data: DataFrame | Mapping[str, Series | ndarray] | None = None,\n    *,\n    compute_se: bool = False,\n) -&gt; typing.Union[numpy.ndarray, pymgcv.custom_types.FitAndSE[numpy.ndarray]]\n</code></pre> <p>Compute the partial effect for a single model term.</p> <p>This method efficiently computes the contribution of one specific term to the model predictions.</p> <p>Parameters:</p> <ul> <li> <code>term</code>               (<code>AbstractTerm | int</code>)           \u2013            <p>The specific term to evaluate (must match a term used in the original model specification) or an integer index representing the position of the term in the target's predictor list</p> </li> <li> <code>target</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Name of the target variable from the keys of <code>gam.predictors</code>. If set to None, the single predictor is used if only one is present, otherwise an error is raised.</p> </li> <li> <code>data</code>               (<code>DataFrame | Mapping[str, Series | ndarray] | None</code>, default:                   <code>None</code> )           \u2013            <p>DataFrame or dictionary containing the variables needed to compute the partial effect for the term.</p> </li> <li> <code>compute_se</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to compute and return standard errors</p> </li> </ul>"},{"location":"api/gam/#pymgcv.gam.AbstractGAM.edf","title":"edf","text":"<pre><code>edf() -&gt; Series\n</code></pre> <p>Compute the effective degrees of freedom (EDF) for the model coefficients.</p> <p>Returns:</p> <ul> <li> <code>Series</code>           \u2013            <p>A series of EDF values, with the mgcv-style coefficient names as the index.</p> </li> </ul>"},{"location":"api/gam/#pymgcv.gam.AbstractGAM.penalty_edf","title":"penalty_edf","text":"<pre><code>penalty_edf()\n</code></pre> <p>Computed the effective degrees of freedom (EDF) associated with each penalty.</p> <p>Returns:</p> <ul> <li>           \u2013            <p>A series of EDF values, with the index being the mgcv-style name of the</p> </li> <li>           \u2013            <p>penalty.</p> </li> </ul>"},{"location":"api/gam/#pymgcv.gam.AbstractGAM.partial_residuals","title":"partial_residuals","text":"<pre><code>partial_residuals(\n    term: AbstractTerm | int,\n    target: str | None = None,\n    data: DataFrame | Mapping[str, Series | ndarray] | None = None,\n    *,\n    avoid_scaling: bool = False,\n) -&gt; ndarray\n</code></pre> <p>Compute partial residuals for model diagnostic plots.</p> <p>Partial residuals combine the fitted values from a specific term with the overall model residuals. They're useful for assessing whether the chosen smooth function adequately captures the relationship, or if a different functional form might be more appropriate.</p> <p>Parameters:</p> <ul> <li> <code>term</code>               (<code>AbstractTerm | int</code>)           \u2013            <p>The model term to compute partial residuals for. If an integer, it is interpreted as the index of the term in the predictor of <code>target</code>.</p> </li> <li> <code>target</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Name of the target variable (response variable or family parameter name from the model specification). If set to None, an error is raised when multiple predictors are present; otherwise, the sole available target is used.</p> </li> <li> <code>data</code>               (<code>DataFrame | Mapping[str, Series | ndarray] | None</code>, default:                   <code>None</code> )           \u2013            <p>A dictionary or DataFrame containing all variables referenced in the model. Defaults to the data used to fit the model.</p> </li> <li> <code>avoid_scaling</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, and the term has a numeric by variable, the scaling by the by variable is not included in the term effect. This facilitates plotting the residuals, as the plots only show the smooth component (unscaled by the by variable).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>Series containing the partial residuals for the specified term</p> </li> </ul>"},{"location":"api/gam/#pymgcv.gam.AbstractGAM.aic","title":"aic","text":"<pre><code>aic(k: float = 2) -&gt; float\n</code></pre> <p>Calculate Akaike's Information Criterion for fitted GAM models.</p> <p>Where possible (fitting <code>GAM</code>/<code>BAM</code> models with \"ML\" or \"REML\"), this uses the approach of Wood, Pya &amp; Saefken 2016, which accounts for smoothing parameter uncertainty, without favouring overly simple models.</p> <p>Parameters:</p> <ul> <li> <code>k</code>               (<code>float</code>, default:                   <code>2</code> )           \u2013            <p>Penalty per parameter (default 2 for classical AIC).</p> </li> </ul>"},{"location":"api/gam/#pymgcv.gam.AbstractGAM.residuals","title":"residuals","text":"<pre><code>residuals(\n    type: Literal[\n        \"deviance\", \"pearson\", \"scaled.pearson\", \"working\", \"response\"\n    ] = \"deviance\",\n)\n</code></pre> <p>Compute the residuals for a fitted model.</p> <p>Parameters:</p> <ul> <li> <code>type</code>               (<code>Literal['deviance', 'pearson', 'scaled.pearson', 'working', 'response']</code>, default:                   <code>'deviance'</code> )           \u2013            <p>Type of residuals to compute, one of:</p> <ul> <li>response: Raw residuals \\(y - \\mu\\), where \\(y\\) is the observed data     and \\(\\mu\\) is the model fitted value.</li> <li>pearson: Pearson residuals \u2014 raw residuals divided by the square     root of the model's mean-variance relationship.     $$     \\frac{y - \\mu}{\\sqrt{V(\\mu)}}     $$</li> <li>scaled.pearson: Raw residuals divided by the standard deviation of     the data according to the model mean variance relationship and     estimated scale parameter.</li> <li>deviance: Deviance residuals as defined by the model\u2019s family.</li> <li>working: Working residuals are the residuals returned from     model fitting at convergence.</li> </ul> </li> </ul>"},{"location":"api/gam/#pymgcv.gam.FitState","title":"FitState","text":"<pre><code>FitState(rgam: ListVector, data: DataFrame | Mapping[str, ndarray | Series])\n</code></pre> <p>The mgcv gam, and the data used for fitting.</p> <p>This gets set as an attribute fit_state on the AbstractGAM object after fitting.</p> <p>Attributes:</p> <ul> <li> <code>rgam</code>           \u2013            <p>The fitted mgcv gam object.</p> </li> <li> <code>data</code>           \u2013            <p>The data used for fitting.</p> </li> </ul>"},{"location":"api/gam/#pymgcv.gam.FitAndSE","title":"FitAndSE","text":"<pre><code>FitAndSE(fit: ~S, se: ~S)\n</code></pre> <p>Container for predictions or partial effects with standard errors.</p> <p>Attributes:</p> <ul> <li> <code>fit</code>           \u2013            <p>Predicted values or partial effect.</p> </li> <li> <code>se</code>           \u2013            <p>Standard errors of the predictions.</p> </li> </ul>"},{"location":"api/miscellaneous/","title":"Miscellaneous API","text":"<p>Various functionality not part of the main API, but we want to be exposed/searchable.</p>"},{"location":"api/miscellaneous/#pymgcv.qq","title":"qq","text":""},{"location":"api/miscellaneous/#pymgcv.qq.QQResult","title":"QQResult","text":"<pre><code>QQResult(theoretical: ndarray, residuals: ndarray, interval: tuple[ndarray, ndarray])\n</code></pre> <p>Results required for a qq plot.</p>"},{"location":"api/miscellaneous/#pymgcv.qq.qq_transform","title":"qq_transform","text":"<pre><code>qq_transform(\n    gam: AbstractGAM,\n    *,\n    transform_to: Literal[\"normal\", \"uniform\"] = \"normal\",\n    level: float = 0.9,\n) -&gt; QQResult\n</code></pre> <p>Generate a QQ-plot by transforming the data to a known distribution.</p> <p>This plots the theoretical quantiles against the transformed data. The data are transformed by 1) passing through the CDF implied by the model, and 2) passing to the quantile function of the distribution implied by <code>transform_to</code>.</p> <p>Note</p> <p>Using <code>transform_to=\"uniform\"</code> may hide outliers/tail behaviour issues,    as the data is constrained to [0, 1], which can be hard to assess visually.</p> <p>Parameters:</p> <ul> <li> <code>gam</code>               (<code>AbstractGAM</code>)           \u2013            <p>The fitted GAM object. The family should support the CDF method, which can be checked with <code>isinstance(family, SupportsCDF)</code>.</p> </li> <li> <code>transform_to</code>               (<code>Literal['normal', 'uniform']</code>, default:                   <code>'normal'</code> )           \u2013            <p>The distribution to transform the residuals to.</p> </li> <li> <code>level</code>               (<code>float</code>, default:                   <code>0.9</code> )           \u2013            <p>The confidence level for the interval.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>QQResult</code> (              <code>QQResult</code> )          \u2013            <p>The results required for a qq plot.</p> </li> </ul>"},{"location":"api/miscellaneous/#pymgcv.qq.qq_simulate","title":"qq_simulate","text":"<pre><code>qq_simulate(\n    gam: AbstractGAM,\n    *,\n    n_sim: int = 50,\n    level: float = 0.9,\n    type: Literal[\"deviance\", \"response\", \"pearson\"] = \"deviance\",\n) -&gt; QQResult\n</code></pre> <p>Generate data for qq-plot via simulation from the family.</p> <p>Parameters:</p> <ul> <li> <code>gam</code>               (<code>AbstractGAM</code>)           \u2013            <p>The fitted GAM object.</p> </li> <li> <code>n_sim</code>               (<code>int</code>, default:                   <code>50</code> )           \u2013            <p>The number of simulations to perform.</p> </li> <li> <code>level</code>               (<code>float</code>, default:                   <code>0.9</code> )           \u2013            <p>The level (i.e. 0.9 means 90% interval).</p> </li> <li> <code>type</code>               (<code>Literal['deviance', 'response', 'pearson']</code>, default:                   <code>'deviance'</code> )           \u2013            <p>The type of residuals to use.</p> </li> </ul>"},{"location":"api/plot/","title":"Plotting","text":"<p><code>pymgcv.plot</code> provides plotting utilities for visualizing GAM models. Plotting is performed using matplotlib. Across the package and examples, we use the import convention <pre><code>import pymgcv.plot as gplt\n</code></pre></p> <ul> <li>For an overall plot of the partial effects of a gam model, use <code>plot</code>.</li> <li>For more fine control over the plotting, specific terms can be plotted onto a single matplotlib axis, using the functions:<ul> <li><code>continuous_1d</code></li> <li><code>continuous_2d</code></li> <li><code>categorical</code></li> <li><code>random_effect</code></li> </ul> </li> </ul>"},{"location":"api/plot/#pymgcv.plot.plot","title":"plot","text":"<pre><code>plot(\n    gam: AbstractGAM,\n    *,\n    ncols: int = 2,\n    scatter: bool = False,\n    data: DataFrame | Mapping[str, ndarray | Series] | None = None,\n    to_plot: type | UnionType | dict[str, list[AbstractTerm]] = AbstractTerm,\n    kwargs_mapper: dict[Callable, dict[str, Any]] | None = None,\n) -&gt; tuple[matplotlib.figure.Figure, matplotlib.axes._axes.Axes | numpy.ndarray]\n</code></pre> <p>Plot a gam model.</p> <p>Except for some specialised cases, this plots the partial effects of the terms.</p> <p>Parameters:</p> <ul> <li> <code>gam</code>               (<code>AbstractGAM</code>)           \u2013            <p>The fitted gam object to plot.</p> </li> <li> <code>ncols</code>               (<code>int</code>, default:                   <code>2</code> )           \u2013            <p>The number of columns before wrapping axes.</p> </li> <li> <code>scatter</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to plot the residuals (where possible), and the overlayed datapoints on 2D plots. For more fine control, see <code>kwargs_mapper</code>. Defaults to False.</p> </li> <li> <code>data</code>               (<code>DataFrame | Mapping[str, ndarray | Series] | None</code>, default:                   <code>None</code> )           \u2013            <p>The data to use for plotting partial residuals and scatter points. Will default to using the data used for fitting. Only relevant if <code>scatter=True</code>.</p> </li> <li> <code>to_plot</code>               (<code>type | UnionType | dict[str, list[AbstractTerm]]</code>, default:                   <code>AbstractTerm</code> )           \u2013            <p>Which terms to plot. If a type, only plots terms of that type (e.g. <code>to_plot = S | T</code> to plot smooths). If a dictionary, it should map the target names to an iterable of terms to plot (similar to how models are specified).</p> </li> <li> <code>kwargs_mapper</code>               (<code>dict[Callable, dict[str, Any]] | None</code>, default:                   <code>None</code> )           \u2013            <p>Used to pass keyword arguments to the underlying <code>pymgcv.plot</code> functions. A dictionary mapping the plotting function to kwargs. For example, to disable the confidence intervals on the 1d plots, set <code>kwargs_mapper</code> to <pre><code>import pymgcv.plot as gplt\n{gplt.continuous_1d: {\"fill_between_kwargs\": {\"disable\": True}}}\n</code></pre></p> </li> </ul>"},{"location":"api/plot/#pymgcv.plot.continuous_1d","title":"continuous_1d","text":"<pre><code>continuous_1d(\n    *,\n    term: AbstractTerm | int,\n    gam: AbstractGAM,\n    target: str | None = None,\n    data: DataFrame | Mapping[str, Series | ndarray] | None = None,\n    eval_density: int = 100,\n    level: str | None = None,\n    n_standard_errors: int | float = 2,\n    residuals: bool = False,\n    plot_kwargs: dict[str, Any] | None = None,\n    fill_between_kwargs: dict[str, Any] | None = None,\n    scatter_kwargs: dict[str, Any] | None = None,\n    ax: Axes | None = None,\n) -&gt; Axes\n</code></pre> <p>Plot 1D smooth or linear terms with confidence intervals.</p> <p>Note</p> <ul> <li>For terms with numeric \"by\" variables, the \"by\" variable is set to 1, showing the unscaled effect of the smooth.</li> </ul> <p>Parameters:</p> <ul> <li> <code>term</code>               (<code>AbstractTerm | int</code>)           \u2013            <p>The model term to plot. Must be a univariate term (single variable). If an integer is provided, it is assumed to be the index of the term in the predictor of <code>target</code>.</p> </li> <li> <code>gam</code>               (<code>AbstractGAM</code>)           \u2013            <p>GAM model containing the term to plot.</p> </li> <li> <code>target</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Name of the target variable (response variable or family parameter name from the model specification). If set to None, an error is raised when multiple predictors are present; otherwise, the sole available target is used.</p> </li> <li> <code>data</code>               (<code>DataFrame | Mapping[str, Series | ndarray] | None</code>, default:                   <code>None</code> )           \u2013            <p>DataFrame used for plotting partial residuals and determining axis limits. Defaults to the data used for training.</p> </li> <li> <code>eval_density</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>Number of evaluation points along the variable range for plotting the smooth curve. Higher values give smoother curves but increase computation time. Default is 100.</p> </li> <li> <code>level</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Must be provided for smooths with a categorical \"by\" variable or a <code>FactorSmooth</code> basis. Specifies the level to plot.</p> </li> <li> <code>n_standard_errors</code>               (<code>int | float</code>, default:                   <code>2</code> )           \u2013            <p>Number of standard errors for confidence intervals.</p> </li> <li> <code>residuals</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to plot partial residuals.</p> </li> <li> <code>plot_kwargs</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Keyword arguments passed to <code>matplotlib.pyplot.plot</code> for the main curve.</p> </li> <li> <code>fill_between_kwargs</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Keyword arguments passed to <code>matplotlib.pyplot.fill_between</code> for the confidence interval band. Pass <code>{\"disable\": True}</code> to disable the confidence interval band.</p> </li> <li> <code>scatter_kwargs</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Keyword arguments passed to <code>matplotlib.pyplot.scatter</code> for partial residuals (ignored if <code>residuals=False</code>).</p> </li> <li> <code>ax</code>               (<code>Axes | None</code>, default:                   <code>None</code> )           \u2013            <p>Matplotlib Axes object to plot on. If None, uses current axes.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Axes</code>           \u2013            <p>The matplotlib Axes object with the plot.</p> </li> </ul>"},{"location":"api/plot/#pymgcv.plot.continuous_2d","title":"continuous_2d","text":"<pre><code>continuous_2d(\n    *,\n    term: AbstractTerm | int,\n    gam: AbstractGAM,\n    target: str | None = None,\n    data: DataFrame | Mapping[str, ndarray | Series] | None = None,\n    eval_density: int = 50,\n    level: str | None = None,\n    contour_kwargs: dict | None = None,\n    contourf_kwargs: dict | None = None,\n    scatter_kwargs: dict | None = None,\n    ax: Axes | None = None,\n) -&gt; Axes\n</code></pre> <p>Plot 2D smooth surfaces as contour plots with data overlay.</p> <p>This function is essential for understanding bivariate relationships and interactions between two continuous variables.</p> <p>Parameters:</p> <ul> <li> <code>term</code>               (<code>AbstractTerm | int</code>)           \u2013            <p>The bivariate term to plot. Must have exactly two variables. Can be S('x1', 'x2') or T('x1', 'x2'). If an integer is provided, it is interpreted as the index of the term the list of predictors for <code>target</code>.</p> </li> <li> <code>gam</code>               (<code>AbstractGAM</code>)           \u2013            <p>GAM model containing the term to plot.</p> </li> <li> <code>target</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Name of the target variable (response variable or family parameter name from the model specification). If set to None, an error is raised when multiple predictors are present; otherwise, the sole available target is used.</p> </li> <li> <code>data</code>               (<code>DataFrame | Mapping[str, ndarray | Series] | None</code>, default:                   <code>None</code> )           \u2013            <p>DataFrame containing the variables for determining plot range and showing data points. Should typically be the training data.</p> </li> <li> <code>eval_density</code>               (<code>int</code>, default:                   <code>50</code> )           \u2013            <p>Number of evaluation points along each axis, creating an eval_density \u00d7 eval_density grid. Higher values give smoother surfaces but increase computation time. Default is 50.</p> </li> <li> <code>level</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Must be provided for smooths with a categorical \"by\" variable or a <code>FactorSmooth</code> basis. Specifies the level to plot.</p> </li> <li> <code>contour_kwargs</code>               (<code>dict | None</code>, default:                   <code>None</code> )           \u2013            <p>Keyword arguments passed to <code>matplotlib.pyplot.contour</code> for the contour lines.</p> </li> <li> <code>contourf_kwargs</code>               (<code>dict | None</code>, default:                   <code>None</code> )           \u2013            <p>Keyword arguments passed to <code>matplotlib.pyplot.contourf</code> for the filled contours.</p> </li> <li> <code>scatter_kwargs</code>               (<code>dict | None</code>, default:                   <code>None</code> )           \u2013            <p>Keyword arguments passed to <code>matplotlib.pyplot.scatter</code> for the data points overlay. Pass <code>{\"disable\": True}</code> to avoid plotting.</p> </li> <li> <code>ax</code>               (<code>Axes | None</code>, default:                   <code>None</code> )           \u2013            <p>Matplotlib Axes object to plot on. If None, uses current axes.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Axes</code>           \u2013            <p>The matplotlib Axes object with the plot, allowing further customization.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the term doesn't have exactly two variables.</p> </li> </ul>"},{"location":"api/plot/#pymgcv.plot.categorical","title":"categorical","text":"<pre><code>categorical(\n    *,\n    term: L | int,\n    gam: AbstractGAM,\n    target: str | None = None,\n    data: DataFrame | Mapping[str, Series | ndarray] | None = None,\n    residuals: bool = False,\n    n_standard_errors: int | float = 2,\n    errorbar_kwargs: dict[str, Any] | None = None,\n    scatter_kwargs: dict[str, Any] | None = None,\n    ax: Axes | None = None,\n) -&gt; Axes\n</code></pre> <p>Plot categorical terms with error bars and partial residuals.</p> <p>Creates a plot showing:</p> <ul> <li>The estimated effect of each category level as points.</li> <li>Error bars representing confidence intervals.</li> <li>Partial residuals as jittered scatter points.</li> </ul> <p>Parameters:</p> <ul> <li> <code>term</code>               (<code>L | int</code>)           \u2013            <p>The categorical term to plot. Must be a L term with a single categorical variable.</p> </li> <li> <code>gam</code>               (<code>AbstractGAM</code>)           \u2013            <p>GAM model containing the term to plot.</p> </li> <li> <code>target</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Name of the target variable (response variable or family parameter name from the model specification). If set to None, an error is raised when multiple predictors are present; otherwise, the sole available target is used.</p> </li> <li> <code>data</code>               (<code>DataFrame | Mapping[str, Series | ndarray] | None</code>, default:                   <code>None</code> )           \u2013            <p>DataFrame (or dictionary) containing the categorical variable and response variable.</p> </li> <li> <code>residuals</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to plot partial residuals (jittered on x-axis).</p> </li> <li> <code>n_standard_errors</code>               (<code>int | float</code>, default:                   <code>2</code> )           \u2013            <p>Number of standard errors for confidence intervals.</p> </li> <li> <code>errorbar_kwargs</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Keyword arguments passed to <code>matplotlib.pyplot.errorbar</code>.</p> </li> <li> <code>scatter_kwargs</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Keyword arguments passed to <code>matplotlib.pyplot.scatter</code>.</p> </li> <li> <code>ax</code>               (<code>Axes | None</code>, default:                   <code>None</code> )           \u2013            <p>Matplotlib Axes object to plot on. If None, uses current axes.</p> </li> </ul>"},{"location":"api/plot/#pymgcv.plot.random_effect","title":"random_effect","text":"<pre><code>random_effect(\n    *,\n    term: S | int,\n    gam: AbstractGAM,\n    target: str | None = None,\n    confidence_interval_level: float = 0.95,\n    axline_kwargs: dict[str, Any] | None = None,\n    scatter_kwargs: dict[str, Any] | None = None,\n    fill_between_kwargs: dict[str, Any] | None = None,\n    ax: Axes | None = None,\n) -&gt; Axes\n</code></pre> <p>A QQ-like-plot for random effect terms.</p> <p>This function plots the estimated random effects against Gaussian quantiles and includes a confidence envelope to assess whether the random effects follow a normal distribution, as assumed by the model.</p> <p>Parameters:</p> <ul> <li> <code>term</code>               (<code>S | int</code>)           \u2013            <p>The random effect term to plot. Must be a smooth term with a <code>RandomEffect</code> basis function. If an integer is provided, it is assumed to be the index of the term in the predictors for <code>target</code>.</p> </li> <li> <code>gam</code>               (<code>AbstractGAM</code>)           \u2013            <p>The fitted GAM model containing the random effect.</p> </li> <li> <code>target</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The target variable to plot when multiple predictors are present. If None and only one predictor exists, that predictor is used.</p> </li> <li> <code>confidence_interval_level</code>               (<code>float</code>, default:                   <code>0.95</code> )           \u2013            <p>The confidence level for the confidence envelope.</p> </li> <li> <code>axline_kwargs</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Keyword arguments passed to <code>matplotlib.axes.Axes.axline</code> for the reference line.</p> </li> <li> <code>scatter_kwargs</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Keyword arguments passed to <code>matplotlib.axes.Axes.scatter</code> for the random effect points.</p> </li> <li> <code>fill_between_kwargs</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Keyword arguments passed to <code>matplotlib.axes.Axes.fill_between</code> for the confidence envelope.</p> </li> <li> <code>ax</code>               (<code>Axes | None</code>, default:                   <code>None</code> )           \u2013            <p>Matplotlib axes to use for the plot. If None, uses the current axes.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Axes</code>           \u2013            <p>The matplotlib axes object.</p> </li> </ul> <p>Note</p> <p>The confidence interval calculation is based on the formula from: \"Worm plot: a simple diagnostic device for modelling growth reference curves\" (page 6). The random effects are constrained to be centered, so the reference line passes through (0, 0).</p>"},{"location":"api/plot/#pymgcv.plot.qq","title":"qq","text":"<pre><code>qq(\n    gam: AbstractGAM,\n    *,\n    qq_fun: Callable[[AbstractGAM], QQResult] = qq_simulate,\n    scatter_kwargs: dict | None = None,\n    fill_between_kwargs: dict | None = None,\n    axline_kwargs: dict | None = None,\n    ax: Axes | None = None,\n) -&gt; Axes\n</code></pre> <p>A Q-Q plot of deviance residuals.</p> <p>Parameters:</p> <ul> <li> <code>gam</code>               (<code>AbstractGAM</code>)           \u2013            <p>The fitted GAM model.</p> </li> <li> <code>qq_fun</code>               (<code>Callable[[AbstractGAM], QQResult]</code>, default:                   <code>qq_simulate</code> )           \u2013            <p>A function taking only the GAM model, and returning a <code>QQResult</code> object storing the theoretical residuals, residuals, and the confidence interval. Defaults to <code>qq_simulate</code>, which is the most widely supported method only requiring the family to provide a sampling function. <code>qq_transform</code> can be used for families providing a cdf method, which transforms the data to a known distribution for which an analytical confidence interval is available.</p> </li> <li> <code>scatter_kwargs</code>               (<code>dict | None</code>, default:                   <code>None</code> )           \u2013            <p>Key word arguments passed to <code>matplotlib.pyplot.scatter</code>.</p> </li> <li> <code>fill_between_kwargs</code>               (<code>dict | None</code>, default:                   <code>None</code> )           \u2013            <p>Key word arguments passed to <code>matplotlib.pyplot.fill_between</code>, for plotting the confidence interval.</p> </li> <li> <code>axline_kwargs</code>               (<code>dict | None</code>, default:                   <code>None</code> )           \u2013            <p>Key word arguments passed to <code>matplotlib.pyplot.axline</code> for plotting the reference line. Pass {\"disable\": True} to avoid plotting.</p> </li> <li> <code>ax</code>               (<code>Axes | None</code>, default:                   <code>None</code> )           \u2013            <p>Matplotlib axes to use for the plot.</p> </li> </ul> <p>Note</p> <p>To change settings of <code>qq_fun</code>, use partial application, e.g. <pre><code>from pymgcv.qq import qq_simulate\nfrom functools import partial\nimport pymgcv.plot as gplt\n\nqq_fun = partial(qq_simulate, level=0.95, n_sim=10)\n# gplt.qq(..., qq_fun=qq_fun)\n</code></pre></p> <p>Returns:</p> <ul> <li> <code>Axes</code>           \u2013            <p>The matplotlib axes object.</p> </li> </ul> <p>Example</p> <p>As an example, we will create a heavy tailed response variable, and fit a <code>Gaussian</code> model, and a <code>Scat</code> model.</p> <pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom pymgcv.families import Gaussian, Scat\nfrom pymgcv.gam import GAM\nimport pymgcv.plot as gplt\nfrom pymgcv.terms import S\n\nrng = np.random.default_rng(1)\nn = 1000\nx = np.linspace(0, 1, n)\ny = np.sin(2 * np.pi * x) + rng.standard_t(df=3, size=n)  # Heavy-tailed\ndata = pd.DataFrame({\"x\": x, \"y\": y})\n\nmodels = [\n    GAM({\"y\": S(\"x\")}, family=Gaussian()),\n    GAM({\"y\": S(\"x\")}, family=Scat()),  # Better for heavy-tailed data\n]\n\nfig, axes = plt.subplots(ncols=2)\n\nfor model, ax in zip(models, axes, strict=False):\n    model.fit(data)\n    gplt.qq(model, ax=ax)\n    ax.set_title(model.family.__class__.__name__)\n    ax.set_box_aspect(1)\n\n# fig.show()  # Uncomment to display the figure\n</code></pre>"},{"location":"api/plot/#pymgcv.plot.residuals_vs_linear_predictor","title":"residuals_vs_linear_predictor","text":"<pre><code>residuals_vs_linear_predictor(\n    gam: AbstractGAM,\n    type: Literal[\n        \"deviance\", \"pearson\", \"scaled.pearson\", \"working\", \"response\"\n    ] = \"deviance\",\n    target: str | None = None,\n    ax: Axes | None = None,\n    scatter_kwargs: dict[str, Any] | None = None,\n)\n</code></pre> <p>Plot the residuals against the linear predictor.</p> <p>Parameters:</p> <ul> <li> <code>gam</code>               (<code>AbstractGAM</code>)           \u2013            <p>The fitted GAM model.</p> </li> <li> <code>type</code>               (<code>Literal['deviance', 'pearson', 'scaled.pearson', 'working', 'response']</code>, default:                   <code>'deviance'</code> )           \u2013            <p>The type of residuals to plot.</p> </li> <li> <code>target</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The target variable to plot residuals for.</p> </li> <li> <code>ax</code>               (<code>Axes | None</code>, default:                   <code>None</code> )           \u2013            <p>The axes to plot on.</p> </li> <li> <code>scatter_kwargs</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Keyword arguments to pass to the scatter plot.</p> </li> </ul>"},{"location":"api/plot/#pymgcv.plot.hexbin_residuals","title":"hexbin_residuals","text":"<pre><code>hexbin_residuals(\n    residuals: ndarray,\n    var1: str,\n    var2: str,\n    data: DataFrame | Mapping[str, ndarray | Series],\n    *,\n    gridsize: int = 25,\n    max_val: int | float | None = None,\n    ax: Axes | None = None,\n    **kwargs: Any,\n)\n</code></pre> <p>Hexbin plot for visualising residuals as function of two variables.</p> <p>Useful e.g. for assessing if interactions are might be required. This is a thin wrapper around <code>matplotlib.pyplot.hexbin</code>, with better defaults for plotting residuals (e.g. uses a symmetric color scale).</p> <p>The default reduction function is <code>np.sum(res) / np.sqrt(len(res))</code>, which has constant variance w.r.t. the number of points.</p> <p>Parameters:</p> <ul> <li> <code>residuals</code>               (<code>ndarray</code>)           \u2013            <p>Residuals to plot.</p> </li> <li> <code>var1</code>               (<code>str</code>)           \u2013            <p>Name of the first variable.</p> </li> <li> <code>var2</code>               (<code>str</code>)           \u2013            <p>Name of the second variable.</p> </li> <li> <code>data</code>               (<code>DataFrame | Mapping[str, ndarray | Series]</code>)           \u2013            <p>The data (containing <code>var1</code> and <code>var2</code>).</p> </li> <li> <code>gridsize</code>               (<code>int</code>, default:                   <code>25</code> )           \u2013            <p>The number of hexagons in the x-direction. The y direction is chosen such that the hexagons are approximately regular.</p> </li> <li> <code>max_val</code>               (<code>int | float | None</code>, default:                   <code>None</code> )           \u2013            <p>Maximum and minimum value for the symmetric color scale. Defaults to the maximum absolute value of the residuals.</p> </li> <li> <code>ax</code>               (<code>Axes | None</code>, default:                   <code>None</code> )           \u2013            <p>Axes to plot on. If None, the current axes are used.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>)           \u2013            <p>Additional keyword arguments passed to <code>matplotlib.hexbin</code>.</p> </li> </ul> <p>Example</p> <pre><code>import numpy as np\nimport pymgcv.plot as gplt\nimport matplotlib.pyplot as plt\n\nrng = np.random.default_rng(1)\n\nfig, ax = plt.subplots()\nresiduals = rng.normal(size=500)  # or gam.residuals()\ndata = {\n    \"x0\": rng.normal(size=residuals.shape),\n    \"x1\": rng.normal(size=residuals.shape),\n    }\ngplt.hexbin_residuals(residuals, \"x0\", \"x1\", data=data, ax=ax)\n</code></pre>"},{"location":"api/terms/","title":"Terms","text":"<ul> <li>Terms are the components of GAM models (e.g. linear, smooths, intercepts etc).</li> <li>If you are familiar with <code>mgcv</code>, then the mgcv representation of the term can be inspected for any term using <code>str(term)</code>.</li> <li>We support adding of terms as syntactic sugar for creating a list of terms, i.e.</li> </ul> <pre><code>from pymgcv.terms import S, L\nassert L(\"x0\") + S(\"x1\") == [L(\"x0\"), S(\"x1\")]\n</code></pre>"},{"location":"api/terms/#pymgcv.terms.L","title":"L","text":"<pre><code>L(name: str)\n</code></pre> <p>Linear (parametric) term with no basis expansion.</p> <ul> <li>If the variable if continuous, the term will be included in the model matrix   as is, with a single corresponding coefficient.</li> <li>If the variable is a categorical variable, the term will be expanded (one-hot   encoded into a set of dummy variables.</li> </ul> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>Name of the variable to include as a linear term.</p> </li> </ul>"},{"location":"api/terms/#pymgcv.terms.S","title":"S","text":"<pre><code>S(\n    *varnames: str,\n    by: str | None = None,\n    k: int | None = None,\n    bs: AbstractBasis | None = None,\n    id: int | None = None,\n    fx: bool = False,\n)\n</code></pre> <p>Smooth term.</p> <p>For all the arguments, passing None will use the mgcv defaults.</p> <p>Note</p> <p>For multiple variables, this creates an isotropic smooth, meaning all variables are treated on the same scale. If variables have very different scales or units, consider using <code>T</code>.</p> <p>Parameters:</p> <ul> <li> <code>*varnames</code>               (<code>str</code>)           \u2013            <p>Names of variables to smooth over. For single variables, creates a univariate smooth. For multiple variables, creates an isotropic multi-dimensional smooth.</p> </li> <li> <code>k</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The dimension of the basis used to represent the smooth term. The default depends on the basis and number of variables that the smooth is a function of. For choosing k, see <code>check_k</code>.</p> </li> <li> <code>bs</code>               (<code>AbstractBasis | None</code>, default:                   <code>None</code> )           \u2013            <p>Basis function. For available options see Basis Functions. If left to none, uses <code>ThinPlateSpline</code>.</p> </li> <li> <code>by</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>variable name used to scale the smooth. If it's a numeric vector, it scales the smooth, and the \"by\" variable should not be included as a seperate main effect (as the smooth is usually not centered). If the \"by\" is a categorical variable, a separate smooth is created for each factor level. In this case the smooths are centered, so the categorical variable should be included as a main effect.</p> </li> <li> <code>id</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Identifier for grouping smooths with shared penalties. If using a categorical by variable, providing an id will ensure a shared smoothing parameter for each level.</p> </li> <li> <code>fx</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Indicates whether the term is a fixed d.f. regression spline (True) or a penalized regression spline (False). Defaults to False.</p> </li> </ul>"},{"location":"api/terms/#pymgcv.terms.T","title":"T","text":"<pre><code>T(\n    *varnames: str,\n    by: str | None = None,\n    k: int | Iterable[int] | None = None,\n    bs: AbstractBasis | Iterable[AbstractBasis] | None = None,\n    d: Iterable[int] | None = None,\n    id: int | None = None,\n    fx: bool = False,\n    np: bool = True,\n    mc: Iterable[bool] | None = None,\n    interaction_only: bool = False,\n)\n</code></pre> <p>Tensor product smooth for scale-invariant multi-dimensional smoothing.</p> <p>Tensor smooths create smooth functions of multiple variables using marginal smooths in order to be robust to variables on different scales. For the sequence arguments, the length must match the number of variables if <code>d</code> is not provided, else they must match the length of <code>d</code>.</p> <p>Parameters:</p> <ul> <li> <code>*varnames</code>               (<code>str</code>)           \u2013            <p>Names of variables for the tensor smooth.</p> </li> <li> <code>k</code>               (<code>int | Iterable[int] | None</code>, default:                   <code>None</code> )           \u2013            <p>The basis dimension for each marginal smooth. If an integer, all marginal smooths will have the same basis dimension. For choosing k, see <code>check_k</code></p> </li> <li> <code>bs</code>               (<code>AbstractBasis | Iterable[AbstractBasis] | None</code>, default:                   <code>None</code> )           \u2013            <p>basis type to use, or an iterable of basis types for each marginal smooth. Defaults to <code>CubicSpline</code></p> </li> <li> <code>d</code>               (<code>Iterable[int] | None</code>, default:                   <code>None</code> )           \u2013            <p>Sequence specifying the dimension of each variable's smooth. For example, (2, 1) would specify to use one two dimensional marginal smooth and one 1 dimensional marginal smooth, where three variables are provided. This is useful for space-time smooths (2 dimensional space and 1 time dimension).</p> </li> <li> <code>by</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Variable name for 'by' variable scaling the tensor smooth, or creating a smooth for each level of a categorical by variable.</p> </li> <li> <code>id</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Identifier for sharing penalties across multiple tensor smooths.</p> </li> <li> <code>fx</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>indicates whether the term is a fixed d.f. regression spline (True) or a penalized regression spline (False). Defaults to False.</p> </li> <li> <code>np</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If False, use a single penalty for the tensor product. If True (default), use separate penalties for each marginal. Defaults to True.</p> </li> <li> <code>mc</code>               (<code>Iterable[bool] | None</code>, default:                   <code>None</code> )           \u2013            <p>If <code>interaction_only</code> is <code>True</code>, this specifies the marginals which should have centering constraints applied. The length should match the number of marginals.</p> </li> <li> <code>interaction_only</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, creates ti() instead of te() - interaction only, excluding main effects of individual variables.</p> </li> </ul>"},{"location":"api/terms/#pymgcv.terms.Interaction","title":"Interaction","text":"<pre><code>Interaction(*varnames: str)\n</code></pre> <p>Parametric interaction term between multiple variables.</p> <p>Any categorical variables involved in an interaction are expanded into indicator variables representing all combinations at the specified interaction order. Numeric variables are incorporated by multiplication (i.e. with eachother and any indicator variables).</p> <p>Note, this does not automatically include main effects or lower order interactions.</p> <p>Parameters:</p> <ul> <li> <code>*varnames</code>               (<code>str</code>)           \u2013            <p>Variable names to include in the interaction. Can be any number of variables.</p> </li> </ul> <p>Example</p> <pre><code># Two-way interaction (multiplication if both numeric)\nfrom pymgcv.terms import Interaction\nage_income = Interaction('age', 'income')\n\n# Three-way interaction\nvarnames = ['group0', 'group1', 'group2']\nthree_way = Interaction(*varnames)\n\n# Generate all pairwise interactions\nfrom itertools import combinations\npairs = [Interaction(*pair) for pair in combinations(varnames, 2)]\n</code></pre> <p>Initialize an interaction term.</p> <p>Parameters:</p> <ul> <li> <code>*varnames</code>               (<code>str</code>)           \u2013            <p>Names of variables to include in the interaction. Must be 2 or more variables.</p> </li> </ul>"},{"location":"api/terms/#pymgcv.terms.Offset","title":"Offset","text":"<pre><code>Offset(name: str)\n</code></pre> <p>Offset term, added to the linear predictor as is.</p> <p>This means:</p> <ul> <li>For log-link models: offset induces a multiplicative effect on the response scale</li> <li>For identity-link models: an offset induces an additive effect on the response     scale</li> </ul> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>Name of the variable to use as an offset. Must be present in the modeling data.</p> </li> </ul>"},{"location":"api/terms/#pymgcv.terms.Intercept","title":"Intercept","text":"<pre><code>Intercept()\n</code></pre> <p>Intercept term.</p> <p>By default, this is added to all formulas in the model. If you want to control the intercept term, then <code>GAM</code> should have <code>add_intercepts</code> set to <code>False</code>, in which case, only intercepts explicitly added will be included in the model.</p>"},{"location":"api/terms/#pymgcv.terms.AbstractTerm","title":"AbstractTerm","text":"<p>Abstract class defining the interface for GAM model terms.</p> <p>Attributes:</p> <ul> <li> <code>varnames</code>           \u2013            <p>Tuple of variable names used by this term. For univariate terms, this contains a single variable name. For multivariate terms (like tensor smooths), this contains multiple variable names.</p> </li> <li> <code>by</code>           \u2013            <p>Optional name of a 'by' variable that scales this term.</p> </li> </ul>"},{"location":"api/terms/#pymgcv.terms.AbstractTerm.label","title":"label","text":"<pre><code>label() -&gt; str\n</code></pre> <p>The label used by pymgcv for the term in plotting and columns.</p> <p>All labels must be unique in a formula. Labels should be implemented such that each unique label must map to a unique mgcv identifier (but not necessarily the other way around).</p>"},{"location":"api/terms/#pymgcv.terms.AbstractTerm.mgcv_identifier","title":"mgcv_identifier","text":"<pre><code>mgcv_identifier(formula_idx: int = 0) -&gt; str\n</code></pre> <p>Generate the mgcv identifier for the term.</p> <p>When computing partial effects, we look for a column with this name from the mgcv output, and if not we fall back on using <code>partial_effect</code>.</p> <p>Example</p> <pre><code>from pymgcv.terms import S\nassert S(\"x1\").mgcv_identifier(formula_idx=1) == \"s.1(x1)\"\n</code></pre> <p>Parameters:</p> <ul> <li> <code>formula_idx</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Index of the formula in multi-formula models.</p> </li> </ul>"},{"location":"api/terms/#pymgcv.terms.AbstractTerm.__str__","title":"__str__","text":"<pre><code>__str__() -&gt; str\n</code></pre> <p>Convert the term to mgcv formula syntax.</p>"},{"location":"","title":"pymgcv: Generalized Additive Models in Python","text":"<p>pymgcv provides a python interface and visualisation tools for R's powerful mgcv library for fitting Generalized Additive Models (GAMs).</p>"},{"location":"#installation","title":"Installation","text":"<p>Warning</p> <p>The package is very new. Expect breaking changes without waning until the package stabilises.</p> <p>Installing the python package only includes the python package and dependencies. This means an R installation with <code>mgcv</code> is also required. Conda and pixi provide two convenient options for handling both Python and R dependencies:</p> <p>Using conda and pip:</p> <ul> <li>Install <code>conda</code> e.g. miniforge</li> </ul> <pre><code>conda create --name my_env python r-base r-mgcv\nconda activate my_env\npython -m pip install pymgcv\n</code></pre> <p>Using pixi:</p> <ul> <li>Install pixi <pre><code>pixi init\npixi add python r-base r-mgcv\npixi add --pypi pymgcv\npixi shell\n</code></pre></li> </ul> <p>Using either method the below example should now run e.g. in the terminal after running <code>python</code>, or in an IDE after selecting the pixi/conda environment</p>"},{"location":"#what-are-gams","title":"What are GAMs?","text":"<p>Generalized Additive Models (GAMs) are a flexible class of statistical models that extend linear models by allowing non-linear relationships between predictors and the response variable. For example the model may have the form</p> <p>$$ g(\\mathbb{E}[Y]) = \\beta_0 + \\sum_{j=1}^p f_j(x_j), $$ Where:</p> <ul> <li>\\(g\\) is the link function, which transforms the expected value of the response variable to a space where modelling with the sum of smooths is reasonable.</li> <li>\\(f_j\\) are smooth functions (e.g. splines) which capture the non-linear relationship between the features and the response.</li> <li>Bivariate \\(f(x_1, x_2)\\) (and multivariate) smooths are also possible, when interactions are important.</li> </ul> <p>Why GAMs?</p> <ul> <li>Flexibility: Capture non-linear relationships automatically</li> <li>Interpretability: The additive nature allows each term to be visualized and understood separately</li> <li>Statistical rigor: Built-in smoothing parameter estimation and uncertainty quantification</li> </ul>"},{"location":"#simple-example","title":"Simple example","text":"<pre><code>import numpy as np\nimport pandas as pd\n\nimport pymgcv.plot as gplt\nfrom pymgcv.gam import GAM\nfrom pymgcv.terms import L, S\n\nrng = np.random.default_rng(1)\nn = 100\nx0, x1, x2, x3 = [rng.uniform(-1, 1, n) for _ in range(4)]\ny = (\n    0.5 * x0 +\n    np.sin(np.pi * x1) +\n    np.cos(np.pi * x2) * np.sin(np.pi * x3) +\n    rng.normal(0, 0.3, n)\n)\ndata = pd.DataFrame({'x0': x0, 'x1': x1, 'x2': x2, 'x3': x3, 'y': y})\n\ngam = GAM({'y': L('x0') + S('x1') + S('x2', 'x3')})\ngam.fit(data)\nfig, ax = gplt.plot(gam, scatter=True, ncols=3) # plot partial effects\nfig.set_size_inches(9, 3)\nfig.show()\n</code></pre> <p>Note</p> <p>Adding terms <code>L(\"x0\") + S(\"x1\")</code> is just syntactic sugar for defining a list of <code>[L(\"x0\"), S(\"x1\")]</code></p>"},{"location":"#what-next","title":"What next?","text":"<p>The key bits of information to explore:</p> <ol> <li>Terms: The types of terms supported by pymgcv (e.g. smooths, linear, interactions, etc)</li> <li>Basis Functions: The available basis functions </li> <li>Families: The available error distribution families and link functions </li> <li>The examples in the side bar!</li> </ol>"},{"location":"examples/electricity_bam/","title":"Electricity modelling with a BAM","text":"<p>This example includes fitting a big data GAM (BAM) to UK electricity demand data, taken from the national grid. The dataset contains:</p> <ul> <li>NetDemand \u2013 net electricity demand</li> <li>wM \u2013 instantaneous temperature, averaged over several cities  </li> <li>wM_s95 \u2013 exponential smooth of <code>wM</code></li> <li>Posan \u2013 periodic index in \\([0, 1]\\) indicating the position along the year  </li> <li>Dow \u2013 factor variable indicating the day of the week </li> <li>Trend \u2013 progressive counter, useful for defining the long-term trend  </li> <li>Instant - The hour \\([0, 23]\\) </li> <li>NetDemand.24 \u2013 24 hour lagged version of <code>NetDemand</code> </li> <li>Year - the year</li> </ul> <pre><code>import matplotlib.pyplot as plt\nimport pandas as pd\n\nimport pymgcv.plot as gplt\nfrom pymgcv.terms import L, S, T\nfrom pymgcv.utils import load_rdata_dataframe_from_url\n\ndata = load_rdata_dataframe_from_url(\n    \"https://github.com/mfasiolo/testGam/raw/master/data/gefcom_big.rda\",\n)\ndata.columns = [c.lower().replace(\".\", \"_\") for c in data.columns]\ndata[\"dow\"] = data[\"dow\"].astype(\"category\")\npd.plotting.scatter_matrix(data.sample(n=250), s=20, figsize=(9,9))\nplt.tight_layout()\n</code></pre> <p></p> <pre><code>from pymgcv.basis_functions import CubicSpline\nfrom pymgcv.gam import BAM\n\ngam1 = BAM(\n    {\n    \"netdemand\":  (\n        L(\"dow\") +   # Encoded as category\n        S(\"netdemand_24\", bs=CubicSpline()) +\n        S(\"trend\", k=6, bs=CubicSpline()) +\n        S(\"wm\", bs=CubicSpline()) +\n        S(\"instant\", bs=CubicSpline()) +\n        S(\"wm_s95\", bs=CubicSpline()) +\n        S(\"posan\", bs=CubicSpline(cyclic=True), k=20)\n    ), \n    }, \n )\n\n\ngam1.fit(data, discrete=True, n_threads=1) \ngplt.plot(gam1)\nplt.show()\n</code></pre> <p></p> <p>Check whether the bases are large enough</p> <pre><code>gam1.check_k()\n</code></pre> <pre><code>term  max_edf        edf   k_index  p_value\n0  s(netdemand_24)      9.0   8.371362  0.976239   0.0475\n1         s(trend)      5.0   4.938627  0.714899   0.0000\n2            s(wm)      9.0   8.961725  1.000205   0.4850\n3       s(instant)      9.0   8.989825  1.016310   0.9075\n4        s(wm_s95)      9.0   8.867216  0.985001   0.1525\n5         s(posan)     18.0  17.049568  0.839946   0.0000\n</code></pre> <p>The p-values for <code>s(trend)</code> and <code>s(posan)</code> are very low:</p> <ul> <li>Raising <code>k</code> for <code>s(posan)</code> may help.</li> <li>Raising <code>k</code> for <code>s(trend)</code> may help, but for time components like this, increasing <code>k</code> to much can lead to overfitting. It might be better to try improving the model in other ways (e.g. autoregressive components).</li> <li>The smooths of <code>instant</code>, <code>wm_s95</code> and <code>Instant</code> have EDF values close to the maximum, so it might help to increase these too.</li> </ul> <pre><code>gam2 = BAM({\n    \"netdemand\":  (\n        L(\"dow\") +   # Encoded as category\n        S(\"netdemand_24\", k=20, bs=CubicSpline()) +\n        S(\"trend\", k=6, bs=CubicSpline()) +\n        S(\"wm\", k=20, bs=CubicSpline()) +\n        S(\"instant\", k=20, bs=CubicSpline()) +\n        S(\"wm_s95\", k=20, bs=CubicSpline()) +\n        S(\"posan\", k=30, bs=CubicSpline(cyclic=True))\n    ), \n    })\ngam2.fit(data, discrete=True)\ngam2.check_k()\n</code></pre> <pre><code>term  max_edf        edf   k_index  p_value\n0  s(netdemand_24)     19.0  11.775060  1.017784   0.8950\n1         s(trend)      5.0   4.939703  0.721719   0.0000\n2            s(wm)     19.0  17.350784  0.976622   0.0725\n3       s(instant)     19.0  18.420370  1.000049   0.5125\n4        s(wm_s95)     19.0  15.928629  1.011876   0.7850\n5         s(posan)     28.0  25.160270  0.828577   0.0000\n</code></pre> <ul> <li>The EDF values are now all somewhat less than the maximum (except from trend, but this is OK).</li> <li>We can check if interaction terms might be useful, for example with <code>instant</code> (the temperature), by looking at the residuals.</li> </ul> <pre><code>residuals = gam2.residuals()\nplot_vars = [\"netdemand_24\", \"wm\", \"wm_s95\", \"posan\"]\nfig, axes = plt.subplots(nrows=2, ncols=2, sharey=True, layout=\"constrained\")\n\nfor var, ax in zip(plot_vars, axes.flatten(), strict=False):\n    gplt.hexbin_residuals(\n        residuals,\n        var,\n        \"instant\",\n        data=data,\n        ax=ax,\n        )\nfig.colorbar(axes[0,0].collections[0], ax=axes);\n</code></pre> <p></p> <p>It's clear that we should include interaction terms! Because the variables are on different scales, we should use <code>T</code>. We can include them as interaction only terms, and keep the original effects.</p> <pre><code>interaction_terms = (\n    T(\"netdemand_24\", \"instant\", k=(15, 15), interaction_only=True) +\n    T(\"wm\", \"instant\", k=(10, 10), interaction_only=True) +\n    T(\"wm_s95\", \"instant\", k=(15, 15), interaction_only=True) +\n    T(\"posan\", \"instant\", k=(10, 15), interaction_only=True)\n)\n# Take previous models predictors and extend the model\npredictors = gam2.predictors.copy()\npredictors[\"netdemand\"].extend(interaction_terms)\ngam3 = BAM(predictors)\ngam3.fit(data, discrete=True)\ngam3.check_k()\n</code></pre> <pre><code>term  max_edf         edf   k_index  p_value\n0           s(netdemand_24)     19.0   13.360622  0.971968   0.0175\n1                  s(trend)      5.0    4.975395  0.654728   0.0000\n2                     s(wm)     19.0   17.582837  0.996571   0.3800\n3                s(instant)     19.0   18.547421  1.015586   0.8875\n4                 s(wm_s95)     19.0   17.492170  0.987718   0.2050\n5                  s(posan)     28.0   26.707128  0.951868   0.0000\n6  ti(netdemand_24,instant)    196.0  103.933439  0.969701   0.0075\n7            ti(wm,instant)     81.0   54.844276  1.004816   0.6500\n8        ti(wm_s95,instant)    196.0  110.628848  0.979752   0.0375\n9         ti(posan,instant)    126.0  113.587174  0.969499   0.0100\n</code></pre> <pre><code>residuals = gam3.residuals()\nfig, axes = plt.subplots(nrows=2, ncols=2, sharey=True, layout=\"constrained\")\n\nfor var, ax in zip(plot_vars, axes.flatten(), strict=False):\n    gplt.hexbin_residuals(residuals, var, \"instant\", data=data, ax=ax)\n\nfig.colorbar(axes[0, 0].collections[0], ax=axes, orientation=\"vertical\", label=\"Residuals\");\n</code></pre> <p></p> <p>Much better! If we check the AIC, it also suggests a better fit </p> <pre><code>for name, m in {\"Univariate smooths\": gam2, \"Interaction\": gam3}.items():\n    print(f\"{name}: {m.aic()}\")\n</code></pre> <pre><code>Univariate smooths: 460416.5326344241\nInteraction: 431496.1108823907\n</code></pre> <pre><code># The smooth and categorical term:\nfig, axes = gplt.plot(gam3, to_plot=S | L, ncols=4)\n</code></pre> <p></p> <ul> <li>We might be concerned with posan - it's quite wiggly! Maybe it's an issue with holidays.</li> </ul> <pre><code># The interaction terms - we'll just show a subset of the datapoints\nfig, axes = gplt.plot(\n    gam3,\n    to_plot=T,\n    scatter=True,\n    data=data.sample(n=400),\n    )\n</code></pre> <p></p>"},{"location":"examples/gamlss/","title":"GAMLSS","text":"<p>GAM models for location scale and shape (GAMLSS) facilitate modelling additional parameters of the response distribution (such as a scale parameter), as a function of the explanatory variables. Below is a simple 1D example:</p> <pre><code>import matplotlib.pyplot as plt\nimport numpy as np\n\n# Create toy dataset\nimport pandas as pd\n\nimport pymgcv.plot as gplt\nfrom pymgcv.families import GauLSS\nfrom pymgcv.gam import GAM\nfrom pymgcv.terms import S\n\nrng = np.random.default_rng(0)\nx = np.linspace(-1, 1, 100)\ndata = pd.DataFrame({\n    \"x\": x,\n    \"y\": rng.normal(loc=np.pi*np.sin(4*x), scale=4*np.abs(x) + 0.1),\n})\n</code></pre> <p>We use the <code>GauLSS</code> family, which supports modelling the log standard deviation</p> <pre><code>gam = GAM(\n    {\n        \"y\": S(\"x\"),\n        \"scale\": S(\"x\"),\n    },\n    family=GauLSS(),\n)\n\ngam.fit(data=data)\npred = gam.predict(data, compute_se=True)\n</code></pre> <p>This gives us a dictionary of targets (response/family parameters) mapping to the fitted values and standard errors, e.g.</p> <pre><code>pred[\"y\"].fit  # numpy vector predicted y values (link scale).\npred[\"y\"].se  # Associated standard errors. \n\npred[\"scale\"].fit  # numpy vector predicted scale values (link scale).\npred[\"scale\"].se;  # Associated standard errors.\n</code></pre> <p>Plotting the partial effects, we can see the model suggests a relationship between \\(x\\) and the response scale</p> <pre><code>gplt.plot(gam)\nplt.show()\n</code></pre> <p></p>"},{"location":"examples/linear_functional/","title":"Linear functional terms","text":"<p>As in mgcv, we can use linear functional terms. This can be achieved by passing matrices of covariates to <code>S</code> or <code>T</code>, in which case the resulting prediction will be the sum of the same smooth, applied to each column of the covariates. For example, for a smooth with two covariates, the \\(i\\)-th prediction would be given by $$ \\sum_{j=1}^J f(x_{ij}, z_{ij}),  $$ where \\(X\\) and \\(Z\\) are \\(n \\times J\\) matrices, and \\(f\\) is the spline. Similarly, if a by matrix \\(B\\) with matching shape is provided as a <code>by</code> variable, this scales the predictions: $$ \\sum_{j=1}^J b_{ij} f(x_{ij}, z_{ij}),  $$</p> <p>We will consider a toy example to predict the number of deaths in a year, in different hospitals, given:</p> <ul> <li>Monthly pollution levels for each hospital (i.e. J=12).</li> <li>Monthly patient loads (as a by variable).</li> </ul> <pre><code>import matplotlib.pyplot as plt\nimport numpy as np\n\nimport pymgcv.plot as gplt\nfrom pymgcv.families import Poisson\nfrom pymgcv.gam import GAM\nfrom pymgcv.terms import S\n\nrng = np.random.default_rng(42)\nn_hospitals, n_months = 500, 12\n\n# Create toy data\npollution = rng.lognormal(mean=1.5, sigma=0.5, size=(n_hospitals, n_months))\nload = rng.uniform(0.5, 1.5, size=(n_hospitals, n_months))\nlog_mu = sum(load[:, m] * 0.2 * np.log1p(pollution[:, m]) for m in range(n_months))\ndeaths = rng.poisson(lam=np.exp(log_mu))\n\ndata = {\n    \"deaths\": deaths,\n    \"pollution\": pollution,\n    \"load\": load,\n}\n\ngam = GAM(\n    {\"deaths\": S(\"pollution\", by=\"load\")},\n    family=Poisson(link=\"log\"),\n)\ngam.fit(data)\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\ngplt.continuous_1d(\n    term=0,\n    gam=gam,\n    ax=axes[0],\n    )\ngplt.qq(gam, ax=axes[1])\naxes[0].set_title(\"Partial effect of pollution on deaths\")\naxes[1].set_title(\"Q-Q plot\");\n</code></pre> <p></p> <p>In this case, the plot shows the shape of the smooth, without summing over columns, and with a weight of 1 (i.e. the by variable is set to 1).</p>"},{"location":"examples/ozone/","title":"Ozone modelling","text":"<p>A practical example of modelling ozone levels, with manual variable selection via p-values and residual checking. The data contains</p> <ul> <li>vh - the height at which the atmospheric pressure is 500mb, in metres</li> <li>wind - the wind speed</li> <li>humidity - Percentage humidity </li> <li>temp - air temperature in Fahrenheit</li> <li>ibh - the inversion layer base height in feet</li> <li>ibt - the inversion base temperature in Fahrenheit</li> <li>dpg - Dagget air pressure gradient in mmhg</li> <li>vis - visibility in miles</li> <li>doy - day of year </li> </ul> <pre><code>import matplotlib.pyplot as plt\n\nimport pymgcv.plot as gplt\nfrom pymgcv.families import Gamma, Gaussian\nfrom pymgcv.gam import GAM\nfrom pymgcv.terms import S\nfrom pymgcv.utils import load_rdata_dataframe_from_url\n\ndata = load_rdata_dataframe_from_url(\n    \"https://github.com/mfasiolo/testGam/raw/master/data/ozone.rda\",\n)\n\ndata.columns = [c.lower().replace(\".\", \"_\") for c in data.columns]\ndata.head()\n\n# 14.9, 18.4, 22.6\n</code></pre> <pre><code>o3    vh  wind  humidity  temp   ibh  dpg  ibt  vis  doy\n1   3  5710     4        28    40  2693  -25   87  250   33\n2   5  5700     3        37    45   590  -24  128  100   34\n3   5  5760     3        51    54  1450   25  139   60   35\n4   6  5720     4        69    35  1568   15  121   60   36\n5   4  5790     6        19    45  2631  -33  123  100   37\n</code></pre> <p>Let's define a basic model for ozone, <code>o3</code>, with Gaussian residuals and log link, using all possible covariates</p> <pre><code>covariates = [colname for colname in data.columns if colname != \"o3\"]\n\ngam_log_link = GAM(\n    {\"o3\": [S(var) for var in covariates]},\n    family=Gaussian(link=\"log\"),\n    )\ngam_log_link.fit(data)\ngplt.plot(gam_log_link, ncols=2);\n</code></pre> <p></p> <pre><code>from rpy2.robjects.packages import importr\n\nfor _i in range(10):\n    stats = importr(\"base\")\n</code></pre> <pre><code># Diagnostic plots\nfig, axes = plt.subplots(ncols=2, layout=\"constrained\")\ngplt.qq(gam_log_link, ax=axes[0])\ngplt.residuals_vs_linear_predictor(gam_log_link, ax=axes[1])\nfig.suptitle(\"Gaussian GAM with log link\");\n</code></pre> <p></p> <p>We can see the residuals get larger with the linear predictor so we can try a gamma response distribution</p> <pre><code>gam_gamma_log_link = GAM(\n    {\"o3\": [S(var) for var in covariates]},\n    family=Gamma(link=\"log\"),\n    )\ngam_gamma_log_link.fit(data)\nfig, axes = plt.subplots(ncols=2, layout=\"constrained\")\ngplt.qq(gam_gamma_log_link, ax=axes[0])\ngplt.residuals_vs_linear_predictor(gam_gamma_log_link, ax=axes[1])\nfig.suptitle(\"Gamma GAM with log link\")\nfig.show()\n</code></pre> <p></p> <p>The residuals look much better! Let's test against the identity link</p> <pre><code>gam_gamma_id_link = GAM(\n    {\"o3\": [S(var) for var in covariates]},\n    family=Gamma(link=\"identity\"),\n    )\ngam_gamma_id_link.fit(data)\n\nmodels = {\n    \"gamma log link\": gam_gamma_log_link,\n    \"gamma identity link\": gam_gamma_id_link,\n}\n\nfor name, model in models.items():\n    print(f\"{name}: AIC = {model.aic():.2f}, total EDF = {sum(model.edf()):.2f}\")\n</code></pre> <pre><code>gamma log link: AIC = 1653.13, total EDF = 25.34\ngamma identity link: AIC = 1677.95, total EDF = 31.45\n</code></pre> <p>The multiplicative model (log link) looks better. Let's consider if we can drop any terms.</p> <pre><code>print(gam_gamma_log_link.summary())\n</code></pre> <pre><code>Family: Gamma \nLink function: log \n\nFormula:\no3 ~ s(vh) + s(wind) + s(humidity) + s(temp) + s(ibh) + s(dpg) + \n    s(ibt) + s(vis) + s(doy) + 1\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.25587    0.01606   140.5   &lt;2e-16 ***\n---\nSignif. codes:  0 \u2018***\u2019 0.001 \u2018**\u2019 0.01 \u2018*\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1\n\nApproximate significance of smooth terms:\n              edf Ref.df      F  p-value    \ns(vh)       1.000  1.000 13.779 0.000245 ***\ns(wind)     1.000  1.000  9.885 0.001830 ** \ns(humidity) 2.483  3.126  2.051 0.097958 .  \ns(temp)     3.279  4.124  3.785 0.004761 ** \ns(ibh)      2.846  3.482  4.717 0.002354 ** \ns(dpg)      4.292  5.375 12.376  &lt; 2e-16 ***\ns(ibt)      1.002  1.004  1.851 0.174647    \ns(vis)      3.000  3.753 10.350  5.3e-07 ***\ns(doy)      5.433  6.624 24.170  &lt; 2e-16 ***\n---\nSignif. codes:  0 \u2018***\u2019 0.001 \u2018**\u2019 0.01 \u2018*\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1\n\nR-sq.(adj) =  0.785   Deviance explained =   83%\n-REML = 848.41  Scale est. = 0.085066  n = 330\n</code></pre> <p><code>ibt</code> has the highest p-value, followed by <code>humidity</code>. Let's consider dropping these</p> <pre><code>models = {\n    \"no_ibt\": GAM(\n        {\"o3\": [S(var) for var in covariates if var != \"ibt\"]},\n        family=Gamma(link=\"log\"),\n    ),\n    \"no_ibt_or_humidity\": GAM(\n        {\"o3\": [S(var) for var in covariates if var not in (\"ibt\", \"humidity\")]},\n        family=Gamma(link=\"log\"),\n    ),\n}\nfor name, model in models.items():\n    model.fit(data)\n    print(f\"{name}: AIC = {model.aic():.2f}, total EDF = {sum(model.edf()):.2f}\")\n</code></pre> <pre><code>no_ibt: AIC = 1652.65, total EDF = 24.32\n</code></pre> <pre><code>no_ibt_or_humidity: AIC = 1653.64, total EDF = 22.19\n</code></pre> <ul> <li>Both models are reasonable, but dropping only <code>ibt</code> gave the model with the lowest AIC. </li> <li>We may favour the model without <code>humidity</code> if we want a simpler model!</li> </ul>"},{"location":"examples/seasonal_co2/","title":"CO2 modelling","text":"<p>This example highlights two aspects of GAM modelling:</p> <ul> <li>Using cyclical smooths for seasonal components</li> <li>The dangers of extrapolation</li> </ul> <p>The data consists of \\(\\text{CO}_2\\) concentration readings, and the cumulative months <code>c.month</code>. We will predict the \\(\\text{CO}_2\\) concentration, and forecast 36 months into the future.</p> <pre><code>import matplotlib.pyplot as plt\nimport numpy as np\n\nfrom pymgcv.basis_functions import CubicSpline\nfrom pymgcv.gam import GAM, AbstractGAM\nfrom pymgcv.terms import S\nfrom pymgcv.utils import load_rdata_dataframe_from_url\n\ndata = load_rdata_dataframe_from_url(\n    \"https://github.com/cran/gamair/raw/master/data/co2s.rda\",\n)\nplt.plot(data[\"c.month\"], data[\"co2\"])\nplt.xlabel(\"Cumulative month\")\nplt.ylabel(r\"CO2 concentration\")\nplt.show()\n</code></pre> <p></p> <p>We'll consider two models, one simple cubic spline model, and one where we use a cyclical component for the month of the year in addition to an overall time component.</p> <p>Note</p> <p>When fitting the cylical model, we provide knots, which ensures that the effect of January is the same as January, not that December and January are the same!</p> <pre><code>simple_gam = GAM({\"co2\": S(\"c.month\", bs=CubicSpline(), k=100)})\nsimple_gam.fit(\n    data[~data.isna().any(axis=1)],\n)\n\ncyclic_gam = GAM(\n    {\n        \"co2\": S(\"c.month\", bs=CubicSpline(), k=50)\n        + S(\"month\", bs=CubicSpline(cyclic=True), k=12),\n    },\n)\n\ncyclic_gam.fit(\n    data[~data.isna().any(axis=1)],\n    knots={\"month\": np.arange(1, 13)},\n    )\n\ndef plot_prediction(gam: AbstractGAM, data, predict_data, ax):\n    predictions = gam.predict(data=predict_data, compute_se=True)\n    ax.plot(data[\"c.month\"], data[\"co2\"], label=\"data\", color=\"black\")\n    ax.plot(\n        predict_data[\"c.month\"],\n        predictions[\"co2\"].fit,\n        label=\"predictions\",\n        color=\"tab:orange\",\n    )\n    ax.fill_between(\n        predict_data[\"c.month\"],\n        predictions[\"co2\"].fit - 2 * predictions[\"co2\"].se,\n        predictions[\"co2\"].fit + 2 * predictions[\"co2\"].se,\n        alpha=0.2,\n        color=\"tab:orange\",\n    )\n\n\npredict_data = {\n    \"c.month\": np.arange(1, 544),\n    \"month\": (np.arange(0, 543) % 12) + 1,\n}\n\nfig, axes = plt.subplots(ncols=2, figsize=(8, 4), sharey=True)\nmodels = {\"Simple GAM\": simple_gam, \"Cyclic GAM\": cyclic_gam}\n\nfor ax, (_name, gam) in zip(axes, models.items(), strict=True):\n    plot_prediction(gam, data, predict_data, ax)\n    ax.set_box_aspect(1)\n\nplt.legend()\nplt.show()\n</code></pre> <p></p> <ul> <li>The simple model without the cyclical component does not extrapolate in any reasonable way!</li> <li>The cyclic model extrapolates reasonably, and likely has a better fit.  </li> </ul>"},{"location":"examples/smooth_by_categorical/","title":"Smooth for each level of categorical variable","text":"<p>To smooth for each level of a categorical variable, there are two primary methods:</p> <ol> <li>Using a categorical by variable</li> <li>Using the <code>FactorSmooth</code> basis function</li> </ol> <p>This example shows both methods. First, we perform the imports and create a toy dataset </p> <pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nimport pymgcv.plot as gplt\nfrom pymgcv.basis_functions import FactorSmooth\nfrom pymgcv.gam import GAM\nfrom pymgcv.terms import L, S\n\nrng = np.random.default_rng(0)\nx = rng.uniform(-1, 1, size=200)\ngroup = pd.Series(\n    rng.choice([\"A\", \"B\", \"C\"], size=len(x)),\n    dtype=pd.CategoricalDtype(categories=[\"A\", \"B\", \"C\"]),\n)\n\ny = rng.normal(scale=0.1, size=len(x))\ny[group == \"A\"] += np.sin(x * 2)[group == \"A\"]\ny[group == \"B\"] += (x**2)[group == \"B\"] + 1 \ny[group == \"C\"] += -(x**2)[group == \"C\"] - 1 \ndata = pd.DataFrame({\"x\": x, \"y\": y, \"group\": group})\n</code></pre>"},{"location":"examples/smooth_by_categorical/#option-1-categorical-by-variables","title":"Option 1: Categorical by variables:","text":"<p><code>S(\"x\", by=\"group\")</code> can be used to fit a seperate smooth for each level of the factor, (or analagously with <code>T</code>):</p> <ul> <li>Such smooths are centered, so we generally must include the categorical variable as a linear term too, i.e. <code>L(\"group\")</code>.</li> <li>If the intercept is left in the model, the partial effect of the first level <code>L(\"group\")</code> will be fixed to zero mean/variance to avoid non-identifiability with the intercept term.</li> <li>Alternativly, we can remove the intercept, in which case all levels of the linear term will have non-zero effects. Below we use this approach by setting <code>add_intercepts=False</code> .</li> </ul> <pre><code>gam = GAM({\"y\": S(\"x\", by=\"group\") + L(\"group\")}, add_intercepts=False)\ngam.fit(data)\nfig, ax = gplt.plot(gam, scatter=True)\nfig.set_size_inches(8, 4)\nfig.show()\n</code></pre> <p></p>"},{"location":"examples/smooth_by_categorical/#option-2-using-factor-smooth-basis","title":"Option 2: Using Factor Smooth basis","text":"<p>In contrast using the <code>FactorSmooth</code> basis leads to:</p> <ol> <li>Sharing a smoothing parameter</li> <li>Non-centered smooths</li> <li>The null space components being penalized (e.g. including level-specific intercepts). The penalization means, this term can be used with an intercept term in the model without introducing any indeterminacy</li> </ol> <pre><code>gam = GAM({\"y\": S(\"x\", \"group\", bs=FactorSmooth())})\nfit = gam.fit(data)\ngplt.plot(gam, scatter=True)\nplt.show()\n</code></pre> <p></p> <p>Note</p> <p>The uncertainty looks much greater for the <code>FactorSmooth</code> approach. This is because it includes uncertainty in the intercept term, whereas the first approach does not. </p>"},{"location":"examples/spatial_fish_eggs/","title":"Mackeral Egg Data","text":"<p>This example gives an example of 2D spatial interactions, using data from a fish egg survey. The key features in the data are:</p> <ul> <li>egg.count - number of eggs found in the net</li> <li>c.dist - distance from 200m seabed contour</li> <li>b.depth - depth of the ocean</li> <li>temp.surf - surface temperature of the ocean</li> <li>temp.20m - water temperature at a depth of 20 meters</li> <li>lat - latitude</li> <li>lon - longitude \u0088- salinity - net.area the area of the net used in m2</li> </ul> <pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nimport pymgcv.plot as gplt\nfrom pymgcv import families as fam\nfrom pymgcv.gam import GAM\nfrom pymgcv.terms import Offset, S\nfrom pymgcv.utils import load_rdata_dataframe_from_url\n\n\ndef get_data():\n    data = load_rdata_dataframe_from_url(\n        \"https://github.com/cran/gamair/raw/master/data/mack.rda\",\n)\n    data.columns = [col.replace(\".\", \"_\") for col in data.columns]\n    data[\"log_net_area\"] = np.log(data[\"net_area\"]) \n    data[\"root_b_depth\"] = data[\"b_depth\"]**0.5\n    data = data.drop(columns=[\"country\", \"vessel\"])\n\n    coast = load_rdata_dataframe_from_url(\n        \"https://github.com/cran/gamair/raw/master/data/coast.rda\",\n)\n\n    # NaN rows split coast data into \"segments\", so we manually seperate them\n    segments = coast.dropna().groupby(coast['lon'].isna().cumsum())\n    return data, segments\n\ndata, segments = get_data()\n\nfig, ax = plt.subplots(figsize=(6, 8))\n\nfor _, seg in segments:\n    ax.plot(seg['lon'], seg['lat'], color=\"k\", linewidth=0.8)\n\nax.set_xlabel(\"Longitude\")\nax.set_ylabel(\"Latitude\")\nax.set_aspect(\"equal\")  # keep correct aspect ratio\n\nax.scatter(\n    data[\"lon\"], data[\"lat\"], s=1+ data[\"egg_count\"], facecolors='none', edgecolors='C0',\n);\n</code></pre> <p></p> <p>Let's look at some key features</p> <pre><code>to_plot = [\n    \"egg_count\",\n    \"b_depth\",\n    \"lat\",\n    \"lon\",\n    \"temp_surf\",\n    \"temp_20m\",\n    \"c_dist\",\n]\n\npd.plotting.scatter_matrix(data[to_plot], figsize=(8, 8))\nplt.show()\n</code></pre> <p></p> <p>We'll try fitting a Poisson GAM.</p> <p>Offsets</p> <p>We know that the number of eggs captured is likely proportional to the net area. We can encode this belief by including an <code>Offset</code> term. Given all the families considered here use a log link function, we can use <code>Offset(\"log_net_area\")</code>.</p> <pre><code>gam1 = GAM({\"egg_count\" : S(\"c_dist\")\n      + S(\"root_b_depth\")\n      + S(\"temp_surf\")\n      + S(\"temp_20m\")\n      + S(\"lat\")\n      + S(\"lon\")\n      + S(\"salinity\")\n      + Offset(\"log_net_area\"), # Account for varying net size!\n      }, family = fam.Poisson())\n\n# The data contains NaNs: subset to only required data, then drop nan rows\ndata = data[gam1.referenced_variables]\ndata = data[~data.isna().any(axis=1)]\ngam1.fit(data=data)\ngplt.qq(gam1)\nplt.show()\n</code></pre> <p></p> <p>That does not look good! The residuals are overdispersed. Maybe a negative binomial or tweedie is better?</p> <pre><code>models = {\n    \"NegativeBinomial\": GAM(gam1.predictors, family=fam.NegativeBinomial()),\n    \"Tweedie\": GAM(gam1.predictors, family=fam.Tw()),\n     }\n\nfig, axes = plt.subplots(ncols=2, layout=\"constrained\")\nfor ax, (name, model) in zip(axes, models.items(), strict=True):\n    model.fit(data=data)\n    gplt.qq(model, ax=ax)\n    ax.set_title(name)\n\nplt.show()\n</code></pre> <p></p> <p>They look better, we'll continue with Tweedie. Let's spatially check the residuals</p> <pre><code>model = models[\"Tweedie\"]\nresiduals = model.residuals()\nmax_val = np.max(np.abs(residuals))  # Set to match on both plots\nax = gplt.hexbin_residuals(residuals, \"lon\", \"lat\", data=data, max_val=max_val)\n\nfor _, seg in segments:\n    ax.plot(seg['lon'], seg['lat'], color=\"k\", linewidth=0.8)\n</code></pre> <p></p> <p>It looks like there is a spatial relationship in the residuals, with regions with predominantly positive or negative residuals. We can try to address this by adding a bivariate smooth for lattitude and longitude</p> <pre><code>bivariate_model = GAM(\n    {\n        \"egg_count\": (\n            S(\"c_dist\")\n            + S(\"root_b_depth\")\n            + S(\"temp_surf\")\n            + S(\"temp_20m\")\n            + S(\"lon\", \"lat\", k=100)\n            + S(\"salinity\")\n            + Offset(\"log_net_area\")\n        ),\n    },\n    family=fam.Tw(),\n)\nbivariate_model.fit(data=data)\nresiduals = bivariate_model.residuals()\n\nax = gplt.hexbin_residuals(residuals, \"lon\", \"lat\", data=data, max_val=max_val) \n\nfor _, seg in segments:\n    ax.plot(seg['lon'], seg['lat'], color=\"k\", linewidth=0.8)\n</code></pre> <p></p> <p>That seems to have helped! Let's take a look at the fitted smooths.</p> <pre><code>fig, axes = gplt.plot(\n    bivariate_model,\n    to_plot=S,\n    )\n\n# Helpful to see datapoints on 2d plot to assess extrapolation risks\naxes[4].scatter(\n    data[\"lon\"], data[\"lat\"], s=0.2, color=\"black\", zorder=2,\n)\n\n# Adjust y-label format\nfor ax in axes.flatten():\n    ax.set_ylabel(ax.get_ylabel().replace(\"~\", \"\\n~\"))\n</code></pre> <p></p>"},{"location":"examples/supplement_vs_placebo/","title":"Supplement vs placebo","text":"<p>The data consists of 112 girls randomized to receive calcium or placebo. The response variable consists of longitudinal measurements of bone mineral density (g/cm\u00b2) measured approximately every 6 months for 3 years. All girls are approximately 11 years old at the start of the trial.  </p> <p>The main variables are:</p> <ul> <li>bmd: bone mass density  </li> <li>group: placebo or supplement  </li> <li>person: factor indicating the ID of each girl  </li> <li>age: the age of each girl at the time of each measurement  </li> </ul> <pre><code>from pymgcv.basis_functions import RandomEffect\nfrom pymgcv.gam import GAM\nfrom pymgcv.terms import L, S\nfrom pymgcv.utils import load_rdata_dataframe_from_url\n\ndata = load_rdata_dataframe_from_url(\n    \"https://github.com/mfasiolo/testGam/raw/master/data/calcium.rda\",\n)\ndata[\"group\"] = data[\"group\"].astype(\"category\")\ndata[\"person\"] = data[\"person\"].astype(\"category\")\n</code></pre> <ul> <li>Let's create a naive model, not taking into account differences between individuals.</li> </ul> <pre><code>gam = GAM({\"bmd\": L(\"group\") + L(\"age\")})\ngam.fit(data)\nprint(gam.summary())\n</code></pre> <pre><code>Family: gaussian \nLink function: identity \n\nFormula:\nbmd ~ group + age + 1\n\nParametric coefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.337882   0.051553   6.554  1.4e-10 ***\ngroupP      -0.019539   0.006055  -3.227  0.00133 ** \nage          0.049501   0.004282  11.560  &lt; 2e-16 ***\n---\nSignif. codes:  0 \u2018***\u2019 0.001 \u2018**\u2019 0.01 \u2018*\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1\n\n\nR-sq.(adj) =  0.221   Deviance explained = 22.4%\n-REML = -625.72  Scale est. = 0.004589  n = 501\n</code></pre> <ul> <li>The placebo has a negative effect, as you would expect if the treatment works</li> <li>Let's add a <code>RandomEffect</code> for each subject</li> </ul> <pre><code>gam = GAM({\"bmd\": L(\"group\") + L(\"age\") + S(\"person\", bs=RandomEffect())}, add_intercepts=False)\ngam.fit(data)\nprint(gam.summary())\n</code></pre> <pre><code>Family: gaussian \nLink function: identity \n\nFormula:\nbmd ~ group + age + s(person, bs = \"re\") - 1\n\nParametric coefficients:\n       Estimate Std. Error t value Pr(&gt;|t|)    \ngroupC 0.329263   0.015168   21.71   &lt;2e-16 ***\ngroupP 0.309940   0.015079   20.55   &lt;2e-16 ***\nage    0.050217   0.001017   49.39   &lt;2e-16 ***\n---\nSignif. codes:  0 \u2018***\u2019 0.001 \u2018**\u2019 0.01 \u2018*\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1\n\nApproximate significance of smooth terms:\n            edf Ref.df     F p-value    \ns(person) 108.4    110 82.75  &lt;2e-16 ***\n---\nSignif. codes:  0 \u2018***\u2019 0.001 \u2018**\u2019 0.01 \u2018*\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1\n\nR-sq.(adj) =   0.96   Deviance explained = 96.9%\n-REML = -1121.3  Scale est. = 0.00023803  n = 501\n</code></pre> <ul> <li>The metrics look much better. Obviously individual differences should be taken into account</li> </ul> <p>Let's consider a non-linear age component, and compare AICs</p> <pre><code>non_lin_gam = GAM({\"bmd\": L(\"group\") + S(\"age\") + S(\"person\", bs=RandomEffect())})\nnon_lin_gam.fit(data)\n\n\nmodels = {\n    \"linear age\": gam,\n    \"non-linear age\": non_lin_gam,\n}\nfor name, model in models.items():\n    print(f\"{name}: AIC={model.aic()}\")\n</code></pre> <pre><code>linear age: AIC=-2659.2059601610404\nnon-linear age: AIC=-2658.1008821828736\n</code></pre> <ul> <li>Very similar AICs, hard to say!</li> </ul> <pre><code>import pymgcv.plot as gplt\n\nfig, ax = gplt.plot(non_lin_gam, ncols=3)\nfig.set_size_inches(8, 2)\n</code></pre> <p></p>"}]}